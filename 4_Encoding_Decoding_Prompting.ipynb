{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7c536ba5e63242ddacba3aec207c15c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_308c92b4159c48fc94b16386197d7793",
              "IPY_MODEL_ecee7b97be8e48aba471dc2ec473b3d7",
              "IPY_MODEL_be01fbba8d57493eb06e0c2b11a0e480"
            ],
            "layout": "IPY_MODEL_4ca03332ba2b4e0fb4a490494eac35e1"
          }
        },
        "308c92b4159c48fc94b16386197d7793": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0776a01061104a85a4129e660ac6115c",
            "placeholder": "​",
            "style": "IPY_MODEL_ee7df535085441f3af1735fb7832a884",
            "value": "Map: 100%"
          }
        },
        "ecee7b97be8e48aba471dc2ec473b3d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ebc5760eefb434db7e5f0aa16b52ad1",
            "max": 7473,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c998d8ba9f59443f9024be07607689d5",
            "value": 7473
          }
        },
        "be01fbba8d57493eb06e0c2b11a0e480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3043b20d583489faf52e75358e8b87c",
            "placeholder": "​",
            "style": "IPY_MODEL_547e4549e4f14205ac5bf9b3bc05ca8f",
            "value": " 7473/7473 [00:00&lt;00:00, 17791.27 examples/s]"
          }
        },
        "4ca03332ba2b4e0fb4a490494eac35e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0776a01061104a85a4129e660ac6115c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee7df535085441f3af1735fb7832a884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ebc5760eefb434db7e5f0aa16b52ad1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c998d8ba9f59443f9024be07607689d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3043b20d583489faf52e75358e8b87c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547e4549e4f14205ac5bf9b3bc05ca8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41b8b9cea0fd4f9f8257ef90b693f62f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f3f35c8dcc646d19b57819ec5611d43",
              "IPY_MODEL_822a0df4e4bc43f1aa4c1de109fc4d9c",
              "IPY_MODEL_f5a5191d48934a6ca546ad1f039b7bc0"
            ],
            "layout": "IPY_MODEL_2eac672d8f2f4351b458dc1c061993a1"
          }
        },
        "5f3f35c8dcc646d19b57819ec5611d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18e0e6abfb68420a81969c8c0e6c8409",
            "placeholder": "​",
            "style": "IPY_MODEL_5423cbad85194848a103f948af97c448",
            "value": "Map: 100%"
          }
        },
        "822a0df4e4bc43f1aa4c1de109fc4d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_027f597841614f7dadf40f6b1d214dc0",
            "max": 1319,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0b0318726bb417ab99d5897b70894c8",
            "value": 1319
          }
        },
        "f5a5191d48934a6ca546ad1f039b7bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc141488f022436e939ef4924372492a",
            "placeholder": "​",
            "style": "IPY_MODEL_3ee53ae4211240c796cd65e4d5fb0a30",
            "value": " 1319/1319 [00:00&lt;00:00, 12702.19 examples/s]"
          }
        },
        "2eac672d8f2f4351b458dc1c061993a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18e0e6abfb68420a81969c8c0e6c8409": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5423cbad85194848a103f948af97c448": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "027f597841614f7dadf40f6b1d214dc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0b0318726bb417ab99d5897b70894c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc141488f022436e939ef4924372492a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ee53ae4211240c796cd65e4d5fb0a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sqkdKB4cVZub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965d852b-f5dc-4132-9d2b-54a579edcf61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Tokenization (15 pts)\n",
        "\n",
        "How does one represent textual input to language models? One strategy that we have seen is to split up words on spaces, e.g.,\n",
        "\n",
        "> This is an example.\n",
        "\n",
        "> [This, is, an, example],\n",
        "\n",
        "but this fails when unseen words appear at test time, e.g.,\n",
        "\n",
        "> We named our son nwonkun.\n",
        "\n",
        "> [We, named, our, son, \\<unk\\>] (5 tokens).\n",
        "\n",
        "One solution to this problem is to use character-level tokens\n",
        "\n",
        "> [W, e, _, n, a, m, e, d, _, o, u, r, _, s, o, n, _, n, w, o, n, k, u, n]\n",
        "\n",
        "(24 tokens, if I counted right), but now the number of tokens required to encode a sentence has increased a *lot*."
      ],
      "metadata": {
        "id": "vRuwwGP_Vaca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Byte-pair encodings and sub-word tokenization (5 pts)\n",
        "\n",
        "[Byte-pair encodings (BPE)](https://en.wikipedia.org/wiki/Byte_pair_encoding) are a clever middle ground for the tokenization problem.\n",
        "Starting from a character-level tokenization, iteratively combine the most common bigrams (token pairs) into their own tokens.\n",
        "For example, the most common bigrams from the previous example are \"_n\" and \"on\". Breaking the tie arbitrarily and creating a new token \"_n\" we now have\n",
        "\n",
        "> [W, e, _n, a, m, e, d, _, o, u, r, _, s, o, n, _n, w, o, n, k, u, n]\n",
        "\n",
        "reducing the token count to 22. Iteratively applying this rule, we can further reduce it to 20 tokens by adding the token \"on\", and so on. Each step of this algorithm greedily reduces the token count by the maximum amount.\n",
        "\n",
        "This tokenization scheme, known as \"sub-word tokenization\" takes the best of both worlds: since the vocabulary still contains tokens for every byte, we never have to use the \\<unk\\> token, while still reducing the number of required tokens to encode a sequence. The more tokens you add, the shorter your sequence gets.\n",
        "\n",
        "To decide which tokens to add to the vocabulary, we have to *train* our BPE tokenizer on a corpus.\n",
        "In this section you will do just that."
      ],
      "metadata": {
        "id": "wr-8Wi_n0HUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "train: str = str.join(\" \", dataset[\"train\"][\"text\"])[:pow(10, 6)]\n",
        "test: str = str.join(\" \", dataset[\"test\"][\"text\"])[:pow(10, 6)]"
      ],
      "metadata": {
        "id": "2AhiO_ZdaHR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6615f5-3f25-469c-a404-43c4e11a017b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import chain, pairwise\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "\n",
        "class Tokenizer:\n",
        "    # The lookup list contains *byte groups*, represented as a tuple of ints.\n",
        "    # The token ID for a byte group is its index in the list.\n",
        "    vocab: list[tuple[int, ...]]\n",
        "\n",
        "    def __init__(self, training_seq: str, vocab_size: int) -> None:\n",
        "        # Initialize a lookup with single-byte groups\n",
        "        self.vocab = [(i,) for i in range(pow(2, 8))]\n",
        "        byte_seq = list(bytes(training_seq, \"utf-8\"))\n",
        "\n",
        "        for _ in tqdm(range(pow(2, 8), vocab_size)):\n",
        "            \"\"\"\n",
        "            TODO: iteratively add the most common token pairs to the vocabulary.\n",
        "            Advice: try using Counter and pairwise from the python std lib.\n",
        "            \"\"\"\n",
        "            pairs = pairwise(byte_seq)\n",
        "            pair_counts = Counter(pairs)\n",
        "\n",
        "            if not pair_counts:\n",
        "                break\n",
        "            most_common_pair = max(pair_counts, key=pair_counts.get)\n",
        "            most_common_pair_token = self.vocab[most_common_pair[0]] + self.vocab[most_common_pair[1]]\n",
        "\n",
        "            self.vocab.append(most_common_pair_token)\n",
        "\n",
        "            i = 0\n",
        "            new_seq = []\n",
        "            while i < len(byte_seq):\n",
        "                if i < len(byte_seq) - 1 and (byte_seq[i], byte_seq[i + 1]) == most_common_pair:\n",
        "                    new_seq.append(len(self.vocab)-1)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_seq.append(byte_seq[i])\n",
        "                    i += 1\n",
        "            byte_seq = new_seq\n",
        "\n",
        "\n",
        "    def tokenize(self, seq: str) -> list[int]:\n",
        "        byte_seq = list(bytes(seq, \"utf-8\"))\n",
        "        token_seq = []\n",
        "        i = 0\n",
        "\n",
        "        while i < len(byte_seq):\n",
        "            same, same_len = None, 0\n",
        "\n",
        "            for j in range(1, min(10, len(byte_seq) - i + 1)):\n",
        "                token_to_find = tuple(byte_seq[i:i + j])\n",
        "                if token_to_find in self.vocab:\n",
        "                    same = self.vocab.index(token_to_find)\n",
        "                    same_len = j\n",
        "\n",
        "            token_seq.append(same if same is not None else byte_seq[i])\n",
        "            i += same_len if same is not None else 1\n",
        "\n",
        "        return token_seq\n",
        "\n",
        "\n",
        "    def detokenize(self, token_seq: list[int]) -> str:\n",
        "        byte_seq = []\n",
        "        for token in token_seq:\n",
        "            byte_seq[len(byte_seq):] = self.vocab[token]\n",
        "        return bytes(byte_seq).decode(\"utf-8\")\n",
        "\n",
        "train_data = train[:10000]\n",
        "tokenizer = Tokenizer(train_data, vocab_size=500)\n",
        "\n",
        "print(\"Some of our new tokens:\")\n",
        "for token in tokenizer.vocab[-10:]:\n",
        "    print(repr(bytes(token).decode(\"utf-8\")))"
      ],
      "metadata": {
        "id": "LxPQF5OFb0cN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b5d52da-a013-4b02-c44f-51dc070b39a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 244/244 [00:02<00:00, 95.76it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Some of our new tokens:\n",
            "'thro'\n",
            "'throug'\n",
            "'pro'\n",
            "'se'\n",
            "'diff'\n",
            "'squad '\n",
            "'batt'\n",
            "'p '\n",
            "'Ar'\n",
            "'Arm'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a sanity check, your implementation should be able to compress the training set to ~40-50% of its original size.\n",
        "You should notice that the test set compression does not perform as well. This is because the distribution of bigrams in the test set does not exactly match the that of the train set. This gets worse the further your test set distribution is from your training set."
      ],
      "metadata": {
        "id": "psiYF10PELk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not edit this code cell\n",
        "test_data = test[:10000]\n",
        "train_bytes_len = len(bytes(train_data, \"utf-8\"))\n",
        "train_token_len = len(tokenizer.tokenize(train_data))\n",
        "print(f\"Compressed train set to {train_token_len / train_bytes_len * 100:.0f}% original size\")\n",
        "test_bytes_len = len(bytes(test_data, \"utf-8\"))\n",
        "test_token_len = len(tokenizer.tokenize(test_data))\n",
        "print(f\"Compressed test set to {test_token_len / test_bytes_len * 100:.0f}% original size\")\n",
        "\n",
        "assert train_data == tokenizer.detokenize(tokenizer.tokenize(train_data))\n",
        "assert test_data == tokenizer.detokenize(tokenizer.tokenize(test_data))"
      ],
      "metadata": {
        "id": "osy9s9wiD2A1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2348ae1c-992c-4b34-e8c3-824e8bb75daf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compressed train set to 43% original size\n",
            "Compressed test set to 52% original size\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 BPE performance on OOD text. (5 pts)\n",
        "\n",
        "Explore how English-trained BPE performs on non-English text by downloading corpora from a few different languages and using your English-trained tokenizer. What do you find? Do the results match your expectations? For what langauges does the tokenizer struggle with the most? How might this impact society if everyone were to use your tokenizer?\n",
        "\n",
        "Include your code, results, and discussion in new cells below.\n",
        "\n",
        "Hint: we recommend you use `load_dataset` to fetch from HuggingFace with `streaming=True` to avoid huge downloads. You might want to take a look at the `oscar` dataset."
      ],
      "metadata": {
        "id": "ZrvZyfOvFqNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "\n",
        "languages = [\"zh\", \"ja\", \"de\", \"ar\", \"es\"]\n",
        "samples = {}\n",
        "\n",
        "for lang in languages:\n",
        "    dataset = load_dataset(\"oscar\", f\"unshuffled_deduplicated_{lang}\", split=\"train\", streaming=True)\n",
        "    samples[lang] = \" \".join([next(iter(dataset))[\"text\"][:500]])\n",
        "\n",
        "tokenizer = Tokenizer(training_seq=train_data, vocab_size=500)\n",
        "for lang, text in samples.items():\n",
        "    print(f\"{lang}: {text[:100]}\")\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"{lang}: Compressed to {len(tokens) / len(bytes(text, 'utf-8')) * 100:.0f}% original size\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "-F_CpvPjHdX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2398f0c-8909-4e2f-b2ce-3ab40432d0a7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 244/244 [00:00<00:00, 250.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "zh: 时间可以被缩短，但过程不可以被省略，只有真正为社会创造价值的企业才能基业长青。大巧不工，重剑无锋，企业最终还是要用业绩和结果说话。一级a卡片在线观看通过产品打磨、团队搭建、市场营销、经营管理等过程，秉\n",
            "zh: Compressed to 100% original size\n",
            "\n",
            "ja: 神社などへ一緒に同行して、様々な角度のショットで家族写真やお子様の写真を撮影致します！お好みに合わせて様々な写真を取ることができますので、その場でカメラマンへのリクエストも可能です！お子様の晴れ姿を、\n",
            "ja: Compressed to 100% original size\n",
            "\n",
            "de: Dosierförderbänder Getriebe Entwässerungssiebmaschine USE 1400 x 3500 mm Eimerkettenbagger Entstaubu\n",
            "de: Compressed to 79% original size\n",
            "\n",
            "ar: مرحبا بك عزيز الزائر نتمنى لك أوقاتاً سعيدة معنا وأن نزداد شرفا بخدمتك ولا تنسى التسجيل معنا لتستفيد\n",
            "ar: Compressed to 99% original size\n",
            "\n",
            "es: Como se librará de la celulitis en el gimnasio La piel superflua en las manos después del adelgazami\n",
            "es: Compressed to 62% original size\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations:\n",
        "\n",
        "- We can see that our tokenizer performs differently on each language just as we expected.\n",
        "- Our tokenizer struggles the most for languages that have characters/alphabets different from the English language like Chinese, Japanese and Arabic. Tokens for these languages hardly get compressed by 1%.\n",
        "- For languages like German and Spanish, which have a lot of similar alphabets when compared to english, the tokenizer performs decently.\n",
        "- Thus, our tokenizer would only perform well if its used on languages that have similar alphabets to the english language.\n",
        "- If we want a universal tokenizer for our society, it must be trained on multilingual corpora, ensuring better support for all languages in a globalized world"
      ],
      "metadata": {
        "id": "4u6CB6fxm0pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Pitfalls of and alternatives to BPE (5 pts)\n",
        "\n",
        "BPE tokenization sufferes from other issues as well. Due to the implementation of our BPE tokenizer, detokenizing a sequence of tokens then re-tokenizing it does not always recover the original sequence:\n",
        "```\n",
        "vocab = {a, aa, b}\n",
        "tokens = [0, 1, 2]\n",
        "detokenized = aaab\n",
        "retokenized = [1, 0, 2]\n",
        "```\n",
        "\n",
        "Another issue is that some tokens that may have been prevalent during BPE training may not be present during language model training, leading to funky situations where the language model has not been trained to represent or output some tokens. See this paper for more information: https://arxiv.org/pdf/2405.05417.\n",
        "\n",
        "Some NLP researchers think that we should move away from sub-word tokenization to get rid of these problems. Engage with this discussion by either\n",
        "- Finding a paper that points out an issue with tokenization and propose your own solution for how you would fix it, or\n",
        "- Finding a paper that proposes an alternative tokenization scheme (or way of processing text) and discuss the drawbacks of the proposed method.\n",
        "\n",
        "Your response should be about a paragraph in length and link to a paper."
      ],
      "metadata": {
        "id": "4wc6vsHwKtiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The paper \"BPE Gets Picky: Efficient Vocabulary Refinement During Tokenizer Training\" by Chizhov et al. (https://arxiv.org/html/2409.04599v1) explores challenges in BPE tokenization, especially the issue of \"junk\" tokens that clutter the vocabulary but rarely get used. To tackle this, the authors introduce \"Picky BPE,\" a refined approach to BPE that filters out low-frequency intermediate tokens during training. This method makes the vocabulary more efficient and reduces the presence of under-trained tokens, which negatively impacts the language model performance. While Picky BPE offers a more compressed vocabulary, it is still based on the traditional subword tokenization technique, meaning it doesn’t completely solve issues like mismatches between detokenization and re-tokenization. Even so, tests show that Picky BPE generally maintains or even enhances model performance, making it a useful improvement over standard BPE."
      ],
      "metadata": {
        "id": "bY9nKjwwxQAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Generation Algorithms (35 pts + 15 pts BONUS)\n",
        "\n",
        "In this problem, we will implement several common decoding algorithms and test them with the GPT-2 Medium model.\n",
        "\n",
        "Given the class below, we will fill in each of the method stubs. You may create additional helper methods as well to make components re-usable.\n",
        "\n",
        "**You are not allowed to use the generate() function in the transformers library. You can only use the model's forward() method to retrieve final layer logits**\n",
        "\n",
        "In addition to the methods we ask you to implement, which are:\n",
        "- Greedy decoding\n",
        "- Temperature Sampling\n",
        "- Nucleus Sampling\n",
        "\n",
        "You will choose ONE of the following sampling algorithms to implement as well (make sure to add your own method, since we do not provide one by default):\n",
        "- Typical Sampling ([Meister et al. (2022)](https://arxiv.org/abs/2202.00666))\n",
        "- Eta Sampling ([Hewitt et al. (2022)](https://arxiv.org/abs/2210.15191))\n",
        "\n",
        "Points for this question will be distributed as follows:\n",
        "\n",
        "- 5-10 points for implementing each decoding algorithm\n",
        "- 5 points for implementing the generate() function (you will make this incrementally through each sub-part)\n",
        "- 5 points for filling out the table with list of tokens (see instructions below)"
      ],
      "metadata": {
        "id": "xxgYpTuExIv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from typing import Optional\n",
        "import numpy as np\n",
        "\n",
        "class LM():\n",
        "    def __init__(self, model_name: str = \"openai-community/gpt2-medium\"):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "\n",
        "    def greedy_decoding(self, prompt: str, max_length: int = 64) -> str:\n",
        "        \"\"\"\n",
        "        TODO:\n",
        "\n",
        "        Implement greedy decoding, in which we use the highest\n",
        "        probability token at each decoding step\n",
        "        \"\"\"\n",
        "        tokens = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        token_ids = tokens.input_ids\n",
        "        logits = self.model(input_ids=token_ids).logits[:, -1, :]\n",
        "        k_scores, k_idxs = torch.topk(logits, k=10, dim=-1)\n",
        "        k_probs = torch.nn.functional.softmax(k_scores, dim=-1)\n",
        "        k_tokens = [self.tokenizer.decode([i]) for i in k_idxs[0]]\n",
        "\n",
        "        print(f\"Top 10 tokens for temperature sampling:\", k_tokens)\n",
        "\n",
        "        for i in range(max_length):\n",
        "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "            token_ids = torch.cat((token_ids, next_token), dim=1)\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "            logits = self.model(input_ids=token_ids).logits[:, -1, :]\n",
        "\n",
        "        return self.tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def temperature_sampling(self, prompt: str, temperature: float = 1.0, max_length: int = 64) -> str:\n",
        "        \"\"\"\n",
        "        TODO:\n",
        "\n",
        "        Implement temperature sampling, in which we sample\n",
        "        from the output distribution at each decoding step,\n",
        "        with a temperature parameter to control the \"peakiness\"\n",
        "        of the output distribution\n",
        "        \"\"\"\n",
        "\n",
        "        tokens = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        token_ids = tokens.input_ids\n",
        "        logits = self.model(input_ids=token_ids).logits[:, -1, :]\n",
        "        logits = logits / temperature\n",
        "        k_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        k_scores, k_idxs = torch.topk(k_probs, k=10, dim=-1)\n",
        "\n",
        "        k_tokens = [self.tokenizer.decode([i]) for i in k_idxs[0]]\n",
        "\n",
        "        print(f\"Top 10 tokens for temperature sampling:\", k_tokens)\n",
        "\n",
        "        for i in range(max_length):\n",
        "            next_token = torch.multinomial(k_probs, num_samples=1)\n",
        "            token_ids = torch.cat((token_ids, next_token), dim=1)\n",
        "            if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "            logits = self.model(input_ids=token_ids).logits[:, -1, :] / temperature\n",
        "            k_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        return self.tokenizer.decode(token_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def nucleus_sampling(self, prompt: str, p: float = 0.9, max_length: int = 64, temperature: float = 1.0) -> str:\n",
        "        \"\"\"\n",
        "        TODO:\n",
        "        Implement nucleus sampling, in which we\n",
        "        sample from a subset of the vocabulary\n",
        "        at each decoding step\n",
        "        Note: There is also a temperature parameter here\n",
        "        \"\"\"\n",
        "        tokens = self.tokenizer(prompt, return_tensors=\"pt\")\n",
        "        token_ids = tokens.input_ids\n",
        "        generated_ids = token_ids.tolist()[0]\n",
        "\n",
        "        for i in range(max_length):\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(input_ids=token_ids).logits[:, -1, :]\n",
        "                logits /= temperature\n",
        "                all_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "                probs, idxs = torch.sort(all_probs, descending=True, dim=-1)\n",
        "                cumulative_probs = torch.cumsum(probs, dim=-1)\n",
        "                mask = cumulative_probs.gt(p)\n",
        "                mask[..., 1:] = mask[..., :-1].clone()\n",
        "                mask[..., 0] = False\n",
        "                probs.masked_fill_(mask, 0.0)\n",
        "\n",
        "                prob_sum = probs.sum(dim=-1, keepdim=True) + 1e-9\n",
        "                normalized_probs = probs / prob_sum\n",
        "\n",
        "                if i == 0:\n",
        "                    top_k = min(10, normalized_probs.size(-1))\n",
        "                    top_k_probs, top_k_idxs = torch.topk(normalized_probs, k=top_k, dim=-1)\n",
        "                    top_k_tokens = [self.tokenizer.decode([idx]) for idx in top_k_idxs[0]]\n",
        "                    print('Top 10 tokens for Nucleus Sampling:', top_k_tokens)\n",
        "\n",
        "                pred_token = idxs[0, torch.multinomial(normalized_probs, num_samples=1).item()].item()\n",
        "                generated_ids.append(pred_token)\n",
        "\n",
        "                if pred_token == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "                token_ids = torch.tensor([generated_ids])\n",
        "\n",
        "        return self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "\n",
        "    def typical_sampling(self, prompt: str, typical_threshold: float = 0.3, max_length: int = 64, epsilon: float = 0.0) -> str:\n",
        "      token_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "      generated_ids = token_ids.tolist()[0]\n",
        "\n",
        "      for i in range(max_length):\n",
        "        logits = self.model(token_ids).logits[:, -1, :]\n",
        "\n",
        "        prob = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        prob_log = -1 * torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "\n",
        "        entropy = -torch.sum(prob * prob_log, dim=-1).unsqueeze(-1)\n",
        "        typicality = torch.abs(prob_log - entropy)\n",
        "\n",
        "\n",
        "        idxs = torch.argsort(typicality, dim=-1, descending=False)\n",
        "\n",
        "        threshold = (typical_threshold * idxs.size(-1))\n",
        "        threshold = torch.tensor(threshold, dtype=torch.int64)\n",
        "\n",
        "        threshold_indices = idxs.narrow(-1, 0, threshold).squeeze(0)\n",
        "\n",
        "        threshold_probs = torch.gather(prob, -1, threshold_indices.unsqueeze(0))\n",
        "        threshold_prob_sum = threshold_probs.sum(dim=-1, keepdim=True) + 1e-9\n",
        "        threshold_probs = threshold_probs / threshold_prob_sum\n",
        "\n",
        "        if i == 0:\n",
        "            top_k = min(10, threshold_probs.size(-1))\n",
        "            top_k_probs, top_k_idxs = torch.topk(threshold_probs, k=top_k, dim=-1)\n",
        "            top_k_tokens = [self.tokenizer.decode([threshold_indices[idx].item()]).strip() for idx in top_k_idxs[0]]\n",
        "            print(\"Top 10 tokens for Typical Sampling\", top_k_tokens)\n",
        "\n",
        "\n",
        "        sampled_index = torch.multinomial(threshold_probs, num_samples=1).item()\n",
        "        pred_token_id = torch.index_select(threshold_indices, 0, torch.tensor([sampled_index])).item()\n",
        "        generated_ids.append(pred_token_id)\n",
        "        if pred_token_id == self.tokenizer.eos_token_id:\n",
        "            break\n",
        "        token_ids = torch.tensor([generated_ids])\n",
        "\n",
        "      return self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    def generate(self,\n",
        "             prompt: str,\n",
        "             temperature: float = 1.0,\n",
        "             p: Optional[float] = None,\n",
        "             max_len: int = 64,\n",
        "             typical_threshold: Optional[float] = None) -> str:\n",
        "        \"\"\"\n",
        "        TODO:\n",
        "\n",
        "        Route to the appropriate generation function\n",
        "        based on the arguments\n",
        "        HINT: What parameter values should map to greedy decoding?\n",
        "        \"\"\"\n",
        "        if temperature == 0:\n",
        "            return self.greedy_decoding(prompt, max_length=max_len)\n",
        "\n",
        "        elif typical_threshold:\n",
        "            return self.typical_sampling(prompt, typical_threshold=typical_threshold, max_length=max_len)\n",
        "\n",
        "        elif p:\n",
        "            return self.nucleus_sampling(prompt, p=p, max_length=max_len, temperature=temperature)\n",
        "\n",
        "        else:\n",
        "            return self.temperature_sampling(prompt, temperature=temperature, max_length=max_len)\n",
        "\n",
        "GPT2LM = LM(\"openai-community/gpt2-medium\")"
      ],
      "metadata": {
        "id": "BR11jNsRqAw3"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each sampling algorithm you implement, fill out this table, in which you will list the top 10 highest probability tokens **at the first decoding step** in a comma separated list. For algorithms like nucleus sampling where you perform some kind of truncation/re-distribution of the output distribution, do the truncation/re-distribution first, and then sort the vocabulary by probability to complete the table.\n",
        "\n",
        "For this and all questions below, use the following prompt:\n",
        "\n",
        "\n",
        "**\"Once upon a time in a land far far away, \"**\n",
        "\n",
        "Note: Use the default value for `max_length` for all questions below."
      ],
      "metadata": {
        "id": "9pJTajqLSq76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Decoding Algorithm** | **10 Highest Probability Tokens** |\n",
        "|------------------------|-----------------------------------|\n",
        "| Greedy                 | [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']                                 |\n",
        "| Temperature (t=1.0)    | [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']                                 |\n",
        "| Nucleus (p=0.9)        | ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']                                |\n",
        "| Typical/Eta            | ['there', 'a', 'the', 'in', '', 'I', 'an', 'when', 'two', 'you']                                 |"
      ],
      "metadata": {
        "id": "t7tJ9bQoTQ8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Once upon a time in a land far far away,\""
      ],
      "metadata": {
        "id": "QObfX94wRWCq"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2LM.generate(prompt, temperature=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "LKdYXWE_YIhn",
        "outputId": "b1637b64-11a3-40c8-ae93-7b54ed9af1a6"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Once upon a time in a land far far away,\\n\\n\\nOur ancestors walked like Zhuangzi \"Leap for joy,\"\\n\\n\\nI Have tried to put on a brave face\\n\\n\\nAnd tell you that I\\'m sorry I can\\'t be here.\\n\\n\\nI\\'ve wandered all over the world and been\\n\\n\\nA young fool of a screen writer\\n\\nAnd'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Greedy Decoding (5 points)\n",
        "\n",
        "First, implement the most simple decoding method of greedy decoding. Here, at each decoding time step, simply use the highest probability token. Note that you'll need to adjust the generate function so that a specific temperature value will map to greedy decoding (what should that value be?).\n",
        "\n",
        "Use the prompt given above to test your implementation. What do you notice?"
      ],
      "metadata": {
        "id": "OpV48G_ez8xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Greedy Decoding Generation:\", GPT2LM.generate(prompt, temperature=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cWx75NnRYmz",
        "outputId": "b9efca6a-3d61-4e0d-9a8a-2df736236253"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Greedy Decoding Generation: Once upon a time in a land far far away, there lived a man named Simeon. He was a wise man, and he knew the secrets of the universe. He knew that the universe was made of many worlds, and that each world was made of a single substance. He knew that each substance was made of a single substance, and that each substance was made\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- It an be seen that the output is very repetitive and lacks creativity in general.\n",
        "- The generated text is highly deterministic, and the model consistently chooses the highest-probability token at each step."
      ],
      "metadata": {
        "id": "iTuogAKRYFPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.2 Temperature Sampling (10 pts)\n",
        "\n",
        "Sometimes (a lot of the time?), we don't actually just want the highest probability token at each time step. Why might this be the case?"
      ],
      "metadata": {
        "id": "xaKx8iVHYZjE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Selecting the highest probability token at each time step can lead to repetitive outputs, particularly in creative tasks like story generation..\n",
        "- Introducing some randomness can produce more varied and engaging results."
      ],
      "metadata": {
        "id": "lH_L6QFjYhzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To adjust for this, we often use sampling algorithms instead of greedy decoding. However, there are many ways we can go about sampling.\n",
        "\n",
        "First, implement temperature sampling. Recall that the temperature parameter adjusts the \"randomness\" of the output at each time step. Here, you'll need to think about how to adjust the output distribution which you will do multinomial sampling from. Be careful about how you will handle very low (close to 0) temperatures.\n",
        "\n",
        "Given the same prompt as above, test your implementation with the following temperature values: [0.3, 0.5, 0.7, 0.9, 1.1]. For each value, sample 3 outputs. What do you notice in terms of the differences between output sets across different temperature values?  "
      ],
      "metadata": {
        "id": "WtwUy0ckIv79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "for i in [0.3, 0.5, 0.7, 0.9, 1.1]:\n",
        "    print(f'For temperature = {i}')\n",
        "    for j in range(3):\n",
        "        print(f'Sample {j+1}: {GPT2LM.generate(prompt, temperature=i)}')\n",
        "        print()"
      ],
      "metadata": {
        "id": "hxI1MULvJppY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "930eb138-52d4-4bd7-89a4-4d8ca4230b41"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For temperature = 0.3\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 1: Once upon a time in a land far far away, a young man1996 was born. He was a boy of eleven. His mother was a woman of the highest rank. She was a very beautiful woman. She was tall and slender, with a beautiful face. She had a very beautiful figure. She was very beautiful. Her hair was long and silky. She had\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 2: Once upon a time in a land far far away, there lived a man who had a great desire to see the world. He was a man of great wisdom, and he had a great desire to see the world. He went to the city of his birth, and there he saw the world. And he said to the gods, \"I want to see the world,\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 3: Once upon a time in a land far far away, there lived a man named Tengen. He was a very wise man, and he had a great many books. One day, he decided to write down all the knowledge he had learned. He began to write down the knowledge he had learned, and he began to write down the knowledge he had learned. He wrote\n",
            "\n",
            "For temperature = 0.5\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 1: Once upon a time in a land far far away, a warrior was summoned by a powerful wizard. The warrior was given a weapon and armor, and instructed to lead a charge against a terrible creature that had been summoned tominimum. The warrior was given the task of slaying the creature, but the wizard had other plans. The wizard was able to summon a number of creatures that\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 2: Once upon a time in a land far far away, there lived a man named Gulliver, who lived in a cave. In the cave he was surrounded by trees, and he was surrounded by trees. One day, he heard a tree falling. He ran from the cave and came to the tree. He fell into the tree and was saved by the tree. He\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 3: Once upon a time in a land far far away, the children of the earth were born to those who were called the sons of God. And those children were called the pure in heart, and the good in heart, and the faithful in word and deed. And those who were called sons of God did not commit adultery, nor steal, nor lie, nor covet.\n",
            "\n",
            "For temperature = 0.7\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 1: Once upon a time in a land far far away, there lived a good-natured man.\n",
            "\n",
            "\n",
            "His name was St. Bernard who was a bishop in the French church and had recently entered into interposition of the Holy See.\n",
            "\n",
            "\n",
            "The bishop (who had been made bishop by his own authority) received the part of being archbishop.\n",
            "\n",
            "\n",
            "St.\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 2: Once upon a time in a land far far away, there was a man named Hazmat. You know the very first person you ever saw wearing a red \"Cancer\" baseball cap? Well, Hazmat was there.\n",
            "\n",
            "Once upon a time in a land far far away, there was a man named Hazmat. You know the very first person you ever saw\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 3: Once upon a time in a land far far away, there was a shard of a man in a white robe who lived in a village. He was a very wise and powerful man, and he kept Flame Torches and stored them away in a dungeon deep within the village. One day, the village chief sensed that someone had broken into the locked dungeon and stole the treasure\n",
            "\n",
            "For temperature = 0.9\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 1: Once upon a time in a land far far away, there lived a wise schoolmaster. He was a close friend of mine, and for hundreds of years, they enjoyed martial arts together. One day, surely, the schoolmaster's life threatened to collapse like a sack of potatoes. He wanted to try out some of the mystic arts, but discovered they weren't as fun\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 2: Once upon a time in a land far far away,\n",
            "\n",
            "The beautiful, the divine, the exalted! East and West,\n",
            "\n",
            "The universal, the intangible!\n",
            "\n",
            "No hair on the tree's upper branch stirs,\n",
            "\n",
            "No echo echoes in the jungle…\n",
            "\n",
            "No gods, no gods! No heaven, no heaven\n",
            "\n",
            "sings in the\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 3: Once upon a time in a land far far away, there lived a young brat with a dream. He dreamed of writing songs. The land had no music. He had wandered long way from home, had ventured far in the cold, and unknowingly discovered a monastery dedicated to the healing arts. That young monk was a mage known as Quodam Oran. He\n",
            "\n",
            "For temperature = 1.1\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 1: Once upon a time in a land far far away, men were poised try doe craunce Isaac boding lain arnieix. color-rich and frhapes i'odegh tife regor bur poogs cancer rigor Hydra Stan chofose an loostkutihe rudo Kamfeaaeead foghdrll le l\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 2: Once upon a time in a land far far away, the dragon queen was murdered–the woman who protected Citizens of Hyzu from Zuko in the past…only to be rejuvenated by her…daughter Princess Na Lu Nsungaserkin. Understandably horrified, of course, Princess Na Lu figured that her sister would be around, and so word swept theDI!\n",
            "\n",
            "Top 10 tokens for temperature sampling: [' there', ' a', ' the', ' in', '\\n', ' I', ' an', ' when', ' two', ' you']\n",
            "Sample 3: Once upon a time in a land far far away, a humble band of moonwars rose up out of cold cockles stalagmite ejecta and glowed military sorcery until their encroaching glory caused Som swirly artillery bombardment across the continent in ship after ship on an impromptu multinational campaign of murderous techno-street vigilantism by frightening NASCAR driver huskies in neck\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Temperature** | **Output 1**                                                                                                                                                                                                                  | **Output 2**                                                                                                                                                                                                                  | **Output 3**                                                                                                                                                                                                                  |\n",
        "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **0.3**         | Once upon a time in a land far far away, a young man1996 was born. He was a boy of eleven. His mother was a woman of the highest rank. She was a very beautiful woman. She was tall and slender, with a beautiful face. She had a very beautiful figure. She was very beautiful. Her hair was long and silky. She had | Once upon a time in a land far far away, there lived a man who had a great desire to see the world. He was a man of great wisdom, and he had a great desire to see the world. He went to the city of his birth, and there he saw the world. And he said to the gods, \"I want to see the world, | Once upon a time in a land far far away, there lived a man named Tengen. He was a very wise man, and he had a great many books. One day, he decided to write down all the knowledge he had learned. He began to write down the knowledge he had learned, and he began to write down the knowledge he had learned. He wrote |\n",
        "| **0.5**         | Once upon a time in a land far far away, a warrior was summoned by a powerful wizard. The warrior was given a weapon and armor, and instructed to lead a charge against a terrible creature that had been summoned tominimum. The warrior was given the task of slaying the creature, but the wizard had other plans. The wizard was able to summon a number of creatures that | Once upon a time in a land far far away, there lived a man named Gulliver, who lived in a cave. In the cave he was surrounded by trees, and he was surrounded by trees. One day, he heard a tree falling. He ran from the cave and came to the tree. He fell into the tree and was saved by the tree. He | Once upon a time in a land far far away, the children of the earth were born to those who were called the sons of God. And those children were called the pure in heart, and the good in heart, and the faithful in word and deed. And those who were called sons of God did not commit adultery, nor steal, nor lie, nor covet. |\n",
        "| **0.7**         | Once upon a time in a land far far away, there lived a good-natured man. His name was St. Bernard who was a bishop in the French church and had recently entered into interposition of the Holy See. The bishop (who had been made bishop by his own authority) received the part of being archbishop. St. | Once upon a time in a land far far away, there was a man named Hazmat. You know the very first person you ever saw wearing a red \"Cancer\" baseball cap? Well, Hazmat was there. Once upon a time in a land far far away, there was a man named Hazmat. You know the very first person you ever saw | Once upon a time in a land far far away, there was a shard of a man in a white robe who lived in a village. He was a very wise and powerful man, and he kept Flame Torches and stored them away in a dungeon deep within the village. One day, the village chief sensed that someone had broken into the locked dungeon and stole the treasure |\n",
        "| **0.9**         | Once upon a time in a land far far away, there lived a wise schoolmaster. He was a close friend of mine, and for hundreds of years, they enjoyed martial arts together. One day, surely, the schoolmaster's life threatened to collapse like a sack of potatoes. He wanted to try out some of the mystic arts, but discovered they weren't as fun | Once upon a time in a land far far away, The beautiful, the divine, the exalted! East and West, The universal, the intangible! No hair on the tree's upper branch stirs, No echo echoes in the jungle… No gods, no gods! No heaven, no heaven sings in the | Once upon a time in a land far far away, there lived a young brat with a dream. He dreamed of writing songs. The land had no music. He had wandered long way from home, had ventured far in the cold, and unknowingly discovered a monastery dedicated to the healing arts. That young monk was a mage known as Quodam Oran. He |\n",
        "| **1.1**         | Once upon a time in a land far far away, men were poised try doe craunce Isaac boding lain arnieix. color-rich and frhapes i'odegh tife regor bur poogs cancer rigor Hydra Stan chofose an loostkutihe rudo Kamfeaaeead foghdrll le l | Once upon a time in a land far far away, the dragon queen was murdered–the woman who protected Citizens of Hyzu from Zuko in the past…only to be rejuvenated by her…daughter Princess Na Lu Nsungaserkin. Understandably horrified, of course, Princess Na Lu figured that her sister would be around, and so word swept theDI! | Once upon a time in a land far far away, a humble band of moonwars rose up out of cold cockles stalagmite ejecta and glowed military sorcery until their encroaching glory caused Som swirly artillery bombardment across the continent in ship after ship on an impromptu multinational campaign of murderous techno-street vigilantism by frightening NASCAR driver huskies in ne |"
      ],
      "metadata": {
        "id": "uu1dyCC0aAuB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lower Temperatures (e.g., 0.3, 0.5): Produce predictable, repetitive, and logically coherent outputs.\n",
        "- Moderate Temperatures (e.g., 0.7): Shows good balance between creativity and coherence, generating diverse yet readable outputs.\n",
        "- Higher Temperatures (e.g., 0.9, 1.1): Prioritizes diversity and creativity but can lead to chaotic, incoherent, or nonsensical outputs."
      ],
      "metadata": {
        "id": "0zLR0i1YXoGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Nucleus Sampling (10 pts)\n",
        "\n",
        "Originally published in [Holtzmann et al. (2021)](https://arxiv.org/abs/1904.09751), nucleus sampling was designed to address an issue that was especially prevalent in language models at the time.\n",
        "\n",
        "This issue is the case of \"neural text degeneration,\" where outputs from LMs would often degenerate into gibberish if a low probability token was ever decoded. To address this, nucleus (also known as top-p) sampling uses a hyperparameter, p, to control how big of a subset of the vocabulary we sample from at each step. For example, if p=0.9, we only sample from the subset of tokens that have a cumulative probability mass of 0.9 (after sorting by probability).\n",
        "\n",
        "Implement nucleus sampling and then use the same prompt as above and test your implementation with the following p-values: [0.97, 0.95, 0.9, 0.8, 0.7]\n",
        "What do you notice across outputs?"
      ],
      "metadata": {
        "id": "z-_Y6K1BJwZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "for p in [0.97, 0.95, 0.9, 0.8, 0.7]:\n",
        "    for j in range(3):\n",
        "        print('Output for p =', p, ':', GPT2LM.generate(prompt, p=p))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsjZ_ZpoQVrH",
        "outputId": "2cdfccca-6228-4e35-b1e0-1c4e47ecabc6"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.97 : Once upon a time in a land far far away, humans travel the land watching over it, watching over rule by perfect government, loyal citizens, and peaceful citizens. In times that have passed, the shadows go as far as to visit this ancient tale, this fear never ceases to infest this people. The voices, whispers, scars, and hallucinations that haunt the most may\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.97 : Once upon a time in a land far far away, a few warriors off the farmland of England suddenly faced an enemy battalion - thirty Orks who were near impossible to kill and so hot on their heels with a war against Chaos at hand that no ordinary army would stand a chance. Suddenly they were prey to snipers, canteens full of contaminated supplies of life feed, stolen\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.97 : Once upon a time in a land far far away, men were violent about their women…\n",
            "\n",
            "During the nineteenth century, tales of female \"aggression\" through the objectification of women reached folkways through the popular media. At the time, domestic violence was uncommon enough that it seemed reasonable to expect \"harmful sex\" as a cause of domestic violence. Meanwhile,\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.95 : Once upon a time in a land far far away, he met Osiris — it was Horus [Hekelspower21] with whom he'd fallen in love before\" (Varro). If Horus is now intimately associated with the Ennead (with whom he previously fell in love), his clearly canonical association with Osiris suggests another related cosmic marriage. This coupling and possibly inter\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.95 : Once upon a time in a land far far away,\n",
            "\n",
            "The world had a star-shaped shape.\n",
            "\n",
            "As a king of armies began his ascension to paradise,\n",
            "\n",
            "The king brought man back with him upon a stool.\n",
            "\n",
            "The king of heaven spoke to man, saying \"Ask the lord to let go of man,\n",
            "\n",
            "Tell him of\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.95 : Once upon a time in a land far far away, there lived a wise man who lived in a village ruled by a bishop called Rubaiyan. A descendant of one of the sons of Rubaiyan, the rich farmer, lived in a village ruled by his sister named Neliairy. And in those days she married an only son, and we all were\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.9 : Once upon a time in a land far far away, there lived a land called Mognirwyn . There, like many, were great swords. In many lands, like many nations, there lived men who made great swords. The people of Mognirwyn made great swords. They were called by their people those who rose to call themselves \"Myclar Wires\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.9 : Once upon a time in a land far far away, there was a king who had lived a long time.\n",
            "\n",
            "He didn't speak to the girl, but she used to keep tabs on him by eavesdropping on his conversations, as she figured he had liked her before.\n",
            "\n",
            "The queen had warned her about such an approach, but the girl had made up her\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.9 : Once upon a time in a land far far away, there lived a man, called Hynek. He was a skilled wielder of swords. I do not recall the source of his knowledge or skills, though. He was brilliant. Amongst his skills, he had mastered flying.[2]\n",
            "\n",
            "It was at Hynek's wedding night that Hynek\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.8 : Once upon a time in a land far far away, there lived a proud and respected wizard and ruler who was dedicated to a simple, albeit dangerous, goal. One day, he was in an unlikely position to succeed as king of his kingdom and this new ruler found himself challenged with a small and troublesome militia of peasant warriors.\n",
            "\n",
            "\n",
            "The young King had already made a name\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.8 : Once upon a time in a land far far away, I was a big boy. All alone and lost in space, I built my home out of junk I picked up when I was a kid. I was unable to tell my mom about it until she called me from the ship. I was scared.\n",
            "\n",
            "The first night she came, I went on tour with her\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.8 : Once upon a time in a land far far away, here is where most things begin. Where there was never a coast or an ocean and seas were always wide, the city was formed.\n",
            "\n",
            "The warrior guild was established in this city to explore and even expand beyond their own lands. Its members would continue to craft the tools of war for decades to come, constantly hoping\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.7 : Once upon a time in a land far far away, the distant moon is fully revealed to you, and with it, the enormous gate.\n",
            "\n",
            "The gate of the gate is locked and locked!\n",
            "\n",
            "The gate of the gate is still locked and locked!\n",
            "\n",
            "The gate of the gate is finally unlocked, and then you see the light of dawn,\n",
            "\n",
            "\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.7 : Once upon a time in a land far far away, there was a race between a wizard and a human.\n",
            "\n",
            "The wizard won.\n",
            "\n",
            "Now that's a lot of magical chivalry.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "The Commander of the Iron Dragon\n",
            "\n",
            "One letter ago, an alien society attempted to destroy the human race. This is how you would call\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "Output for p = 0.7 : Once upon a time in a land far far away, there lived a ruler called Anomen and his court. The king was a powerful warrior, a true warrior. He was of noble blood and, despite his great power, was often in need of help. One day he called upon a well-known scholar of his age, Alanna the Wise, to guide him through\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **p**  | **Output 1**                                                                                                                                                                                                                          | **Output 2**                                                                                                                                                                                                                          | **Output 3**                                                                                                                                                                                                                          |\n",
        "|--------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| 0.97   | Once upon a time in a land far far away, humans travel the land watching over it, watching over rule by perfect government, loyal citizens, and peaceful citizens. In times that have passed, the shadows go as far as to visit this ancient tale, this fear never ceases to infest this people. The voices, whispers, scars, and hallucinations that haunt the most may | Once upon a time in a land far far away, a few warriors off the farmland of England suddenly faced an enemy battalion - thirty Orks who were near impossible to kill and so hot on their heels with a war against Chaos at hand that no ordinary army would stand a chance. Suddenly they were prey to snipers, canteens full of contaminated supplies of life feed, stolen | Once upon a time in a land far far away, men were violent about their women… During the nineteenth century, tales of female \"aggression\" through the objectification of women reached folkways through the popular media. At the time, domestic violence was uncommon enough that it seemed reasonable to expect \"harmful sex\" as a cause of domestic violence. Meanwhile,   |\n",
        "| 0.95   | Once upon a time in a land far far away, he met Osiris — it was Horus [Hekelspower21] with whom he'd fallen in love before\" (Varro). If Horus is now intimately associated with the Ennead (with whom he previously fell in love), his clearly canonical association with Osiris suggests another related cosmic marriage. This coupling and possibly inter | Once upon a time in a land far far away, The world had a star-shaped shape. As a king of armies began his ascension to paradise, The king brought man back with him upon a stool. The king of heaven spoke to man, saying \"Ask the lord to let go of man, Tell him of | Once upon a time in a land far far away, there lived a wise man who lived in a village ruled by a bishop called Rubaiyan. A descendant of one of the sons of Rubaiyan, the rich farmer, lived in a village ruled by his sister named Neliairy. And in those days she married an only son, and we all were |\n",
        "| 0.9    | Once upon a time in a land far far away, there lived a land called Mognirwyn . There, like many, were great swords. In many lands, like many nations, there lived men who made great swords. The people of Mognirwyn made great swords. They were called by their people those who rose to call themselves \"Myclar Wires | Once upon a time in a land far far away, there was a king who had lived a long time. He didn't speak to the girl, but she used to keep tabs on him by eavesdropping on his conversations, as she figured he had liked her before. The queen had warned her about such an approach, but the girl had made up her | Once upon a time in a land far far away, there lived a man, called Hynek. He was a skilled wielder of swords. I do not recall the source of his knowledge or skills, though. He was brilliant. Amongst his skills, he had mastered flying. It was at Hynek's wedding night that Hynek |\n",
        "| 0.8    | Once upon a time in a land far far away, there lived a proud and respected wizard and ruler who was dedicated to a simple, albeit dangerous, goal. One day, he was in an unlikely position to succeed as king of his kingdom and this new ruler found himself challenged with a small and troublesome militia of peasant warriors. The young King had already made a name | Once upon a time in a land far far away, I was a big boy. All alone and lost in space, I built my home out of junk I picked up when I was a kid. I was unable to tell my mom about it until she called me from the ship. I was scared. The first night she came, I went on tour with her | Once upon a time in a land far far away, here is where most things begin. Where there was never a coast or an ocean and seas were always wide, the city was formed. The warrior guild was established in this city to explore and even expand beyond their own lands. Its members would continue to craft the tools of war for decades to come, constantly hoping |\n",
        "| 0.7    | Once upon a time in a land far far away, the distant moon is fully revealed to you, and with it, the enormous gate. The gate of the gate is locked and locked! The gate of the gate is still locked and locked! The gate of the gate is finally unlocked, and then you see the light of dawn, | Once upon a time in a land far far away, there was a race between a wizard and a human. The wizard won. Now that's a lot of magical chivalry. Sincerely, The Commander of the Iron Dragon One letter ago, an alien society attempted to destroy the human race. This is how you would call | Once upon a time in a land far far away, there lived a ruler called Anomen and his court. The king was a powerful warrior, a true warrior. He was of noble blood and, despite his great power, was often in need of help. One day he called upon a well-known scholar of his age, Alanna the Wise, to guide him through |"
      ],
      "metadata": {
        "id": "5TJnTe2LcjnY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For p = 0.97 and 0.95, outputs are highly creative but often incoherent with  sudden shifts in themes.  \n",
        "- At p = 0.9, there’s a good balance between creativity and coherence, producing engaging yet structured outputs.  \n",
        "- For p = 0.8 and 0.7, outputs are more predictable and structured, sacrificing creativity for consistency.  "
      ],
      "metadata": {
        "id": "42kgPc4UXsHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 More variations on decoding algorithms (10 pts)\n",
        "\n",
        "Nucleus sampling was definitely not the end of the road in terms of new decoding algorithms. Even in the past few years, new decoding algorithms have been proposed to address some limitations of existing algorithms.\n",
        "\n",
        "Two in particular are:\n",
        "- Typical Sampling ([Meister et al. (2022)](https://arxiv.org/abs/2202.00666))\n",
        "- Eta Sampling ([Hewitt et al. (2022)](https://arxiv.org/abs/2210.15191))\n",
        "\n",
        "For this question, CHOOSE ONE of the two algorithms presented above. Below, please describe in a few sentences what your chosen algorithm does in a novel way and the broad motivation behind it. Along with this description, present 3 sampled outputs for the same prompt as above (you can use one hyperparameter value for all of these).\n",
        "\n"
      ],
      "metadata": {
        "id": "nsIqM7tAQ3U-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typical Sampling:\n",
        "\n",
        "Novelty:  \n",
        "\n",
        "*   Typical Sampling adjusts the sampling threshold dynamically, focusing on tokens with high probability but also allowing diversity.\n",
        "*   The key idea is to adjust the sampling range based on the probability of the tokens, ensuring that tokens from the \"typical\" region of the distribution are more likely to be selected.\n",
        "*   This prevents the model from choosing outliers too frequently while maintaining coherence.\n",
        "\n",
        "\n",
        "\n",
        "Motivation:\n",
        "\n",
        "*   It aims to generate more meaningful and diverse text by focusing on the \"typical\" region of the token distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "gwkhi_7rWZTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO\n",
        "for i in range(3):\n",
        "    print('Output for Typical Sampling', i, ':', GPT2LM.generate(prompt, typical_threshold=0.5, max_len=50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-MSG09rhp7v",
        "outputId": "ad8145cf-9fa1-4b08-c31a-a14e3de47417"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 tokens for Typical Sampling ['there', 'a', 'the', 'in', '', 'I', 'an', 'when', 'two', 'you']\n",
            "Output for Typical Sampling 0 : Once upon a time in a land far far away, teeming with strange vitality... \" (Marvel Comics Single-Page Comic - Fall 1994/Spring/Summer 1995 10th Anniversary Issue) [14][15]\n",
            "\n",
            "Tom LaPille and Roz Alanis (According to \"Joe Quesada\n",
            "Top 10 tokens for Typical Sampling ['there', 'a', 'the', 'in', '', 'I', 'an', 'when', 'two', 'you']\n",
            "Output for Typical Sampling 1 : Once upon a time in a land far far away, the gods greeted the People of the West and the Elves. The Peoples thanked them, saying 'As they do for all things of ours, so shall angels enable the Promised Land to triumph over these woes!' Nevertheless there appeared devils on the Plain.\n",
            "Top 10 tokens for Typical Sampling ['there', 'a', 'the', 'in', '', 'I', 'an', 'when', 'two', 'you']\n",
            "Output for Typical Sampling 2 : Once upon a time in a land far far away, there lived a powerful town, in whose peaks we set our tents. There lived Kinan Datin [sic] a folk-tale. Two youths crept up in the night sing [that] the King had held for three nights in the hidden town\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Decoding Method**       | **Output 1**                                                                                                                                  | **Output 2**                                                                                                                                                      | **Output 3**                                                                                                                                                      |\n",
        "|---------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| **Typical Sampling**       | Once upon a time in a land far far away, teeming with strange vitality... \" (Marvel Comics Single-Page Comic - Fall 1994/Spring/Summer 1995 10th Anniversary Issue) [14][15]  | Once upon a time in a land far far away, the gods greeted the People of the West and the Elves. The Peoples thanked them, saying 'As they do for all things of ours, so shall angels enable the Promised Land to triumph over these woes!' Nevertheless there appeared devils on the Plain. | Once upon a time in a land far far away, there lived a powerful town, in whose peaks we set our tents. There lived Kinan Datin [sic] a folk-tale. Two youths crept up in the night sing [that] the King had held for three nights in the hidden town |"
      ],
      "metadata": {
        "id": "H3SLtnxudeh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 BONUS (Up to 15 pts)\n",
        "\n",
        "Can you find a prompt where the continuations do not differ much across multiple sampling strategies, even when we use high temperatures or high p values? (Hint: Think about overfitting)"
      ],
      "metadata": {
        "id": "0iIkmLhfWtEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_prompt = 'dont judge a book'\n",
        "\n",
        "print(GPT2LM.generate(new_prompt, max_len = 3))\n",
        "print(GPT2LM.generate(new_prompt, temperature=0.7,max_len = 3))\n",
        "print(GPT2LM.generate(new_prompt, temperature=0.95, max_len = 3))\n",
        "print(GPT2LM.generate(new_prompt, p=0.7, max_len = 3))\n",
        "print(GPT2LM.generate(new_prompt, p=0.9, max_len = 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvhK69JsRD4e",
        "outputId": "9804a7b2-f708-47dc-eb41-f4affc9f8ea5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 tokens for temperature sampling: [' by', ' until', ' for', ' based', ' on', ',', ' that', '.', ' before', ' just']\n",
            "dont judge a book of religion you\n",
            "Top 10 tokens for temperature sampling: [' by', ' until', ' for', ' based', ' on', ',', ' that', '.', ' before', ' just']\n",
            "dont judge a book by its cover\n",
            "Top 10 tokens for temperature sampling: [' by', ' until', ' for', ' based', ' on', ',', ' that', '.', ' before', ' just']\n",
            "dont judge a book by its cover\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '%', ')', '*', '(', '$', '\"', '&', '#', \"'\"]\n",
            "dont judge a book by its cover\n",
            "Top 10 tokens for Nucleus Sampling: ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']\n",
            "dont judge a book by its cover\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Prompting (50 pts)\n",
        "\n",
        "In this problem, we will try various prompting approaches and prompt an LLM for a Math Reasoning Benchmark called [GSM8K](https://github.com/openai/grade-school-math), which contains grade school math word problems. This is a very common _reasoning_ benchmark used to test various LLMs.\n",
        "\n",
        "The LLM that we will be using is [Google Gemini](https://gemini.google.com/). We will be prompting Gemini by using an API call to the Gemini Model. Normally, you can also prompt Open Source LLMs via the HuggingFace Library, however due to compute constraints, we use Gemini in this problem."
      ],
      "metadata": {
        "id": "kweZjWF3LpfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the GSM8K Dataset and Google Gemini\n",
        "\n",
        "Follow the steps below to download the GSM8K Dataset and to setup Google Gemini on Colab. You will automatically get points for this subpr"
      ],
      "metadata": {
        "id": "56o8lVgVNVXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"gsm8k\", 'main')"
      ],
      "metadata": {
        "id": "Cg_sNCLeL5j0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset['train']), len(dataset['test'])"
      ],
      "metadata": {
        "id": "fcI6_53zN5if",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da58bf7f-f4e0-42b8-a01c-192948322c21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7473, 1319)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# An example instance of this dataset\n",
        "\n",
        "dataset['test'][6]"
      ],
      "metadata": {
        "id": "0zfGgqwZN9EN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f67ddfbe-9ebb-4d61-fc07-d340adc6d22f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do Toulouse, Charleston, and Seattle have together if Seattle has 20 sheep?',\n",
              " 'answer': 'If Seattle has 20 sheep, Charleston has 4 * 20 sheep = <<20*4=80>>80 sheep\\nToulouse has twice as many sheep as Charleston, which is 2 * 80 sheep = <<2*80=160>>160 sheep\\nTogether, the three has 20 sheep + 160 sheep + 80 sheep = <<20+160+80=260>>260 sheep\\n#### 260'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train'][8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFYv5y7LZzQG",
        "outputId": "aa5b7072-efb4-4b91-ca39-10983defe0ee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'Alexis is applying for a new job and bought a new set of business clothes to wear to the interview. She went to a department store with a budget of $200 and spent $30 on a button-up shirt, $46 on suit pants, $38 on a suit coat, $11 on socks, and $18 on a belt. She also purchased a pair of shoes, but lost the receipt for them. She has $16 left from her budget. How much did Alexis pay for the shoes?',\n",
              " 'answer': 'Let S be the amount Alexis paid for the shoes.\\nShe spent S + 30 + 46 + 38 + 11 + 18 = S + <<+30+46+38+11+18=143>>143.\\nShe used all but $16 of her budget, so S + 143 = 200 - 16 = 184.\\nThus, Alexis paid S = 184 - 143 = $<<184-143=41>>41 for the shoes.\\n#### 41'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gemini Setup (from the official [Gemini documentation](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/get-started/python.ipynb))\n",
        "\n",
        "\n",
        "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
        "\n",
        "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>\n",
        "\n",
        "In Colab, add the key to the secrets manager under the \"🔑\" in the left panel.\n",
        "\n",
        "---\n",
        "\n",
        "Give it the name `GEMINI_API_KEY`.\n",
        "\n",
        "Once you have the API key, pass it to the SDK. You can do this in two ways:\n",
        "\n",
        "* Put the key in the `GEMINI_API_KEY` environment variable (the SDK will automatically pick it up from there).\n",
        "* Pass the key to `genai.configure(api_key=...)`"
      ],
      "metadata": {
        "id": "CY-vwQI5OmPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# All imports for this question\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from datasets import Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "from typing import Callable, List, Any"
      ],
      "metadata": {
        "id": "QlcHWlpQOIh6"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "YZOA3PVIPWxD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test if your setup is working, do not change the model name\n",
        "model = genai.GenerativeModel(\"gemini-1.0-pro\")\n",
        "response = model.generate_content(\"What is Natural Language Processing? Explain it to a five year old.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "Cv8B6I2lPiMw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "6783ab9e-448f-48d7-ed7b-5606b719dd51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have a very special friend called a \"computer\" who can understand what you say and write, just like your mom or dad. This special friend is always learning new words and trying to figure out what you mean by what you say. It's called Natural Language Processing, which is like a secret code that helps computers understand us humans!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Data and Prompting Setup (15 + 5 pts)\n",
        "\n",
        "In this part, we will create some boilerplate code to process our dataset and generate prompts from the dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "DAAyTvXeQ5jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing the GSM8K Dataset"
      ],
      "metadata": {
        "id": "4njT8xp2apCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_gsm8k_answers(dataset: Dataset) -> Dataset:\n",
        "    \"\"\"\n",
        "    Processes the GSM8K dataset to remove reasoning chains and retain only the numerical answers.\n",
        "    Assumes answers are separated from reasoning by the '###' string.\n",
        "\n",
        "    Args:\n",
        "    dataset (Dataset): Huggingface Dataset object for GSM8K.\n",
        "\n",
        "    Returns:\n",
        "    Dataset: Processed Dataset object with numerical answers only.\n",
        "    \"\"\"\n",
        "\n",
        "    def extract_answer(sample):\n",
        "        # IMPLEMENT HERE\n",
        "        # Split the answer using '###' and return a dictionary with the key 'processed_answer'\n",
        "\n",
        "        return {'processed_answer': sample['answer'].split('####')[-1].strip()}\n",
        "\n",
        "    return dataset.map(extract_answer)"
      ],
      "metadata": {
        "id": "6IDGGocfQldW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Prompts (15 pts)\n",
        "\n",
        "We will be implementing FIVE (5) prompting methods. See their descriptions below -\n",
        "1. **Zero-Shot Answer Only (2 pts)**: You prompt the model to only generate the answer to the question\n",
        "\n",
        "2. **Zero-Shot Chain of Thought (CoT) (3 pts)**: Refer to the [Chain of Thought Paper](https://arxiv.org/abs/2201.11903). CoT refers to a reasoning chain that is generated by the model before generating the actual answer. This has shown to improve performance. In this setup, you will prompt the model to generate a reasoning chain before the answer.\n",
        "\n",
        "3. **5-Shot Answer Only (2 pts)**: You provide some in-context examples to prompt the model with to generate the answer. This is analogous to Approach 1. Use the a random set of 5 examples from the training set to create the in-context examples.\n",
        "\n",
        "4. **5-Shot CoT (3 pts)**: Combine Approaches 2 and 3 to do 5-shot CoT prompting.\n",
        "\n",
        "5. **Your own prompt! (5 pts)**: Try something new. Think about how you solve Math problems and implement your own prompting method."
      ],
      "metadata": {
        "id": "9ddnQ6b4azH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_zero_shot(problem: str) -> str:\n",
        "    \"\"\"\n",
        "    Zero-shot prompt.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    return f\"Question: {problem}\\nAnswer:\""
      ],
      "metadata": {
        "id": "MiW-lHk1aniq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_zero_shot_cot(problem: str) -> str:\n",
        "    \"\"\"\n",
        "    Zero-shot Chain of Thought (CoT) prompt.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    return f\"Question: {problem}\\nLet's think step by step:\\nAnswer:\""
      ],
      "metadata": {
        "id": "8gSts3jebOLp"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_5_shot(problem: str, training_set: Dataset) -> str:\n",
        "    \"\"\"\n",
        "    5-shot prompt generation for GSM8K problems. Randomly selects 5 examples from the training set.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt with 5 in-context_examples.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    examples = np.random.choice(list(training_set), 5, replace=False)\n",
        "\n",
        "\n",
        "    prompt = \"\"\n",
        "    for example in examples:\n",
        "        prompt += f\"Example Question: {example['question']}\\Solution: {example['answer']}\\n\\n\"\n",
        "\n",
        "    prompt += f\"Question: {problem}\\nAnswer:\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "G1RQZKU-bbi4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prompt_generation_5_shot_cot(problem: str, training_set: Dataset) -> str:\n",
        "    \"\"\"\n",
        "    5-shot Chain of Thought (CoT) prompt generation. Randomly selects 5 examples\n",
        "    from the training set and includes reasoning steps.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt with 5 CoT in-context examples.\n",
        "    \"\"\"\n",
        "    # IMPLEMENT HERE\n",
        "    examples = np.random.choice(list(training_set), 5, replace=False)\n",
        "\n",
        "    prompt = \"\"\n",
        "    for example in examples:\n",
        "        prompt += f\"Example Question: {example['question']}\\Let's think step by step:\\n{example['answer']} Answer: {example['processed_answer']}\\n\\n\"\n",
        "\n",
        "    prompt += f\"Question: {problem}\\nLet's think step by step:\\nAnswer:\"\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "UNteVGTWbnLW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datasets import Dataset\n",
        "\n",
        "# # Feel free to change the method definition\n",
        "\n",
        "def my_prompt(problem: str, training_set: Dataset) -> str:\n",
        "    \"\"\"\n",
        "    Your own unique way of prompting an LLM for Math word problems.\n",
        "\n",
        "    Returns:\n",
        "    str: The generated prompt\n",
        "    \"\"\"\n",
        "    examples = np.random.choice(list(training_set), 5, replace=False)\n",
        "\n",
        "    prompt = \"\"\n",
        "    for example in examples:\n",
        "        prompt += f\"Example Question: {example['question']}\\Reason and Solution:\\n{example['answer']}\\nAnswer: {example['processed_answer']}\\n\\n\"\n",
        "\n",
        "    prompt += f\"Problem: {problem}\\nLet's solve this problem step-by-step to ensure accuracy:\\n\"\n",
        "    prompt += \"1. Identify the relevant formula.\\n\"\n",
        "    prompt += \"2. Substitute known values into the formula.\\n\"\n",
        "    prompt += \"3. Perform the necessary calculations.\\n\"\n",
        "    prompt += \"4. Double-check the result for accuracy.\\nAnswer:\"\n",
        "\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "xGPpBU29tBva"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Prompting Gemini and Implementing Self-Consistency (5 + 5 + 10 pts)\n",
        "\n",
        "Here, you will help build the wrapper for prompting Gemini using the prompt methods you have designed above.\n",
        "\n",
        "You will then also implement Self-Consistency based prompting. Refer to the [Self-Consistency Paper](https://arxiv.org/abs/2203.11171). In order to implement Self-Consistency, you generate multiple Zero-Shot CoT (Approach 2 in the prompting methods) candidates, and take a majority vote of the answers predicted by each candidate."
      ],
      "metadata": {
        "id": "j4uARh18cXDH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First, write the function where you will process the answer generated by the model. (5 pts)\n",
        "\n",
        "Note that answer processing changes for different prompt types, so this function also takes in the name of the method in its argument."
      ],
      "metadata": {
        "id": "XuVhq91Oc5TB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Any, List\n",
        "\n",
        "def answer_processing(prediction: str, prompt_function: Any) -> str:\n",
        "    \"\"\"\n",
        "    Processes the model's generated output to extract the final answer.\n",
        "\n",
        "    Returns:\n",
        "    str: The processed numerical answer.\n",
        "    \"\"\"\n",
        "    prediction = prediction.replace(',', '')\n",
        "    answer = prediction.strip().split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    try:\n",
        "        answer = round(float(re.findall(r\"[-+]?\\d*\\.?\\d+\", answer)[-1]))\n",
        "    except:\n",
        "        answer = 0\n",
        "\n",
        "    return str(answer)"
      ],
      "metadata": {
        "id": "PYqUnKoxy2Jg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not change, method to calculate accuracy from predictions and ground truth labels\n",
        "\n",
        "def evaluate_accuracy(predictions: List[str], ground_truths: List[str]) -> float:\n",
        "    correct = 0\n",
        "    total = len(predictions)\n",
        "\n",
        "    for pred, true in zip(predictions, ground_truths):\n",
        "        if pred == true:\n",
        "            correct += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    return accuracy * 100"
      ],
      "metadata": {
        "id": "hWOHGuWzUF2Z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Next, write the wrapper function where you use all the building blocks constructed above to prompt the Gemini model (5 + 10 pts)\n",
        "\n",
        "\n",
        "On how to prompt Gemini, refer to the [Gemini Text Generation Handbook](https://ai.google.dev/gemini-api/docs/text-generation?lang=python).\n",
        "\n",
        "Hint: Reading this will help you figure out how to generate multiple candidates to implement Self-Consistency."
      ],
      "metadata": {
        "id": "cbXg7blmfZeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Callable, List\n",
        "from datasets import Dataset\n",
        "from collections import Counter\n",
        "import time\n",
        "\n",
        "def pipeline_generate(\n",
        "    model_instance: Any,\n",
        "    training_set: Dataset,\n",
        "    test_set: Dataset,\n",
        "    prompt_function: Callable[[str], str],\n",
        "    process_answer_function: Callable[[str, Callable], str],\n",
        "    evaluation_function: Callable[[List[str], List[str]], float],\n",
        "    self_consistency: int,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    model_instance (Any): The Google Gemini model instance.\n",
        "    test_set (Dataset): The GSM8K test set to evaluate on.\n",
        "    prompt_function (Callable): Function to generate prompts for the test set.\n",
        "    process_answer_function (Callable): Function to process the model's generated answers.\n",
        "    evaluation_function (Callable): Function to evaluate model's answers against the ground truth.\n",
        "    self_consistency: Number of samples to run self-consistency approach on.\n",
        "    If negative, 0 or 1, this implies regular prompting\n",
        "\n",
        "    Returns:\n",
        "    float: The accuracy of the model on the test set.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    ground_truths = test_set['processed_answer']\n",
        "\n",
        "    prompts_with_examples = ['prompt_generation_5_shot', 'prompt_generation_5_shot_cot', 'my_prompt']\n",
        "\n",
        "    for problem in test_set['question']:\n",
        "\n",
        "        if prompt_function.__name__ in prompts_with_examples:\n",
        "            prompt = prompt_function(problem, training_set)\n",
        "        else:\n",
        "            prompt = prompt_function(problem)\n",
        "\n",
        "        if self_consistency > 1:\n",
        "            answers = []\n",
        "            for i in range(self_consistency):\n",
        "                response = model_instance.generate_content(prompt).text\n",
        "                processed_answer = process_answer_function(response, prompt_function)\n",
        "                answers.append(processed_answer)\n",
        "                time.sleep(5)\n",
        "            answer_counts = Counter(answers)\n",
        "            final_answer = answer_counts.most_common(1)[0][0]\n",
        "\n",
        "        else:\n",
        "            response = model_instance.generate_content(prompt).text\n",
        "            final_answer = process_answer_function(response, prompt_function)\n",
        "\n",
        "        predictions.append(final_answer)\n",
        "        time.sleep(5)\n",
        "\n",
        "    accuracy = evaluation_function(predictions, ground_truths)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "jejQ-ubFz39J"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_train_processed = process_gsm8k_answers(dataset['train'])\n",
        "gsm8k_test_processed = process_gsm8k_answers(dataset['test'])\n",
        "\n",
        "\n",
        "\n",
        "# The following line is just to test your systems, comment this line out to report results on the entire test set in 3.3\n",
        "gsm8k_train_processed = Dataset.from_dict(gsm8k_train_processed[:1000])\n",
        "gsm8k_test_processed = Dataset.from_dict(gsm8k_test_processed[:50])\n",
        "\n",
        "accuracy = pipeline_generate(\n",
        "    model_instance=model,\n",
        "    training_set = gsm8k_train_processed,\n",
        "    test_set=gsm8k_test_processed,\n",
        "    prompt_function=prompt_generation_zero_shot,\n",
        "    process_answer_function=answer_processing,\n",
        "    evaluation_function=evaluate_accuracy,\n",
        "    self_consistency=1,\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "id": "tzHoeMnyUIYp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "53a86b1c-f559-47e8-b998-266100541439"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 44.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_train_processed = process_gsm8k_answers(dataset['train'])\n",
        "gsm8k_test_processed = process_gsm8k_answers(dataset['test'])\n",
        "\n",
        "\n",
        "# The following line is just to test your systems, comment this line out to report results on the entire test set in 3.3\n",
        "gsm8k_train_processed = Dataset.from_dict(gsm8k_train_processed[:1000])\n",
        "gsm8k_test_processed = Dataset.from_dict(gsm8k_test_processed[:50])\n",
        "\n",
        "accuracy = pipeline_generate(\n",
        "    model_instance=model,\n",
        "    training_set = gsm8k_train_processed,\n",
        "    test_set=gsm8k_test_processed,\n",
        "    prompt_function=prompt_generation_zero_shot_cot,\n",
        "    process_answer_function=answer_processing,\n",
        "    evaluation_function=evaluate_accuracy,\n",
        "    self_consistency=1,\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "90McoZop_RcB",
        "outputId": "5df6e018-52dc-4c9f-ed9f-71a45be98307"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 70.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_train_processed = process_gsm8k_answers(dataset['train'])\n",
        "gsm8k_test_processed = process_gsm8k_answers(dataset['test'])\n",
        "\n",
        "\n",
        "# The following line is just to test your systems, comment this line out to report results on the entire test set in 3.3\n",
        "gsm8k_train_processed = Dataset.from_dict(gsm8k_train_processed[:1000])\n",
        "gsm8k_test_processed = Dataset.from_dict(gsm8k_test_processed[:50])\n",
        "\n",
        "accuracy = pipeline_generate(\n",
        "    model_instance=model,\n",
        "    training_set = gsm8k_train_processed,\n",
        "    test_set=gsm8k_test_processed,\n",
        "    prompt_function=prompt_generation_5_shot,\n",
        "    process_answer_function=answer_processing,\n",
        "    evaluation_function=evaluate_accuracy,\n",
        "    self_consistency=1,\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "7c536ba5e63242ddacba3aec207c15c7",
            "308c92b4159c48fc94b16386197d7793",
            "ecee7b97be8e48aba471dc2ec473b3d7",
            "be01fbba8d57493eb06e0c2b11a0e480",
            "4ca03332ba2b4e0fb4a490494eac35e1",
            "0776a01061104a85a4129e660ac6115c",
            "ee7df535085441f3af1735fb7832a884",
            "4ebc5760eefb434db7e5f0aa16b52ad1",
            "c998d8ba9f59443f9024be07607689d5",
            "f3043b20d583489faf52e75358e8b87c",
            "547e4549e4f14205ac5bf9b3bc05ca8f",
            "41b8b9cea0fd4f9f8257ef90b693f62f",
            "5f3f35c8dcc646d19b57819ec5611d43",
            "822a0df4e4bc43f1aa4c1de109fc4d9c",
            "f5a5191d48934a6ca546ad1f039b7bc0",
            "2eac672d8f2f4351b458dc1c061993a1",
            "18e0e6abfb68420a81969c8c0e6c8409",
            "5423cbad85194848a103f948af97c448",
            "027f597841614f7dadf40f6b1d214dc0",
            "d0b0318726bb417ab99d5897b70894c8",
            "fc141488f022436e939ef4924372492a",
            "3ee53ae4211240c796cd65e4d5fb0a30"
          ]
        },
        "id": "LTy15kb5_UWo",
        "outputId": "79785689-7ac6-4057-ca16-7b3906883789"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c536ba5e63242ddacba3aec207c15c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41b8b9cea0fd4f9f8257ef90b693f62f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 70.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_train_processed = process_gsm8k_answers(dataset['train'])\n",
        "gsm8k_test_processed = process_gsm8k_answers(dataset['test'])\n",
        "\n",
        "\n",
        "# The following line is just to test your systems, comment this line out to report results on the entire test set in 3.3\n",
        "gsm8k_train_processed = Dataset.from_dict(gsm8k_train_processed[:1000])\n",
        "gsm8k_test_processed = Dataset.from_dict(gsm8k_test_processed[:50])\n",
        "\n",
        "accuracy = pipeline_generate(\n",
        "    model_instance=model,\n",
        "    training_set = gsm8k_train_processed,\n",
        "    test_set=gsm8k_test_processed,\n",
        "    prompt_function=prompt_generation_5_shot_cot,\n",
        "    process_answer_function=answer_processing,\n",
        "    evaluation_function=evaluate_accuracy,\n",
        "    self_consistency=1,\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "XsJaO2t1_Wr5",
        "outputId": "1d810c95-7ca5-40f5-817f-109b514c9372"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 72.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_train_processed = process_gsm8k_answers(dataset['train'])\n",
        "gsm8k_test_processed = process_gsm8k_answers(dataset['test'])\n",
        "\n",
        "\n",
        "# The following line is just to test your systems, comment this line out to report results on the entire test set in 3.3\n",
        "gsm8k_train_processed = Dataset.from_dict(gsm8k_train_processed[:1000])\n",
        "gsm8k_test_processed = Dataset.from_dict(gsm8k_test_processed[:50])\n",
        "\n",
        "accuracy = pipeline_generate(\n",
        "    model_instance=model,\n",
        "    training_set = gsm8k_train_processed,\n",
        "    test_set=gsm8k_test_processed,\n",
        "    prompt_function=my_prompt,\n",
        "    process_answer_function=answer_processing,\n",
        "    evaluation_function=evaluate_accuracy,\n",
        "    self_consistency=1,\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "P1esDQiw_YVx",
        "outputId": "70c4b716-3085-4528-8368-664585eba44a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 70.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gsm8k_train_processed = process_gsm8k_answers(dataset['train'])\n",
        "gsm8k_test_processed = process_gsm8k_answers(dataset['test'])\n",
        "\n",
        "\n",
        "# The following line is just to test your systems, comment this line out to report results on the entire test set in 3.3\n",
        "gsm8k_train_processed = Dataset.from_dict(gsm8k_train_processed[:1000])\n",
        "gsm8k_test_processed = Dataset.from_dict(gsm8k_test_processed[:50])\n",
        "\n",
        "accuracy = pipeline_generate(\n",
        "    model_instance=model,\n",
        "    training_set = gsm8k_train_processed,\n",
        "    test_set=gsm8k_test_processed,\n",
        "    prompt_function=prompt_generation_zero_shot,\n",
        "    process_answer_function=answer_processing,\n",
        "    evaluation_function=evaluate_accuracy,\n",
        "    self_consistency=5,\n",
        ")\n",
        "\n",
        "print(f\"Accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "oaWaMwqA_azh",
        "outputId": "50180014-e4bb-4a2a-ef15-999d1c5d3830"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 50.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Complete this table based on your implementation in 3.2 and answer the following questions (5 + 5 pts)"
      ],
      "metadata": {
        "id": "NPt0dz1lgpZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Round each value up to two decimal points (5 pts)\n",
        "\n",
        "Method|Accuracy\n",
        "---|---|\n",
        "0-shot| 44%\n",
        "0-shot CoT| 70%\n",
        "5-shot| 70%\n",
        "5-shot CoT| 72%\n",
        "My prompt| 70%\n",
        "0-shot CoT Self-Consistency| 50%"
      ],
      "metadata": {
        "id": "3rSganJ6gz8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What was the intuition behind the prompt that you designed? (2 pts)\n",
        "\n",
        "The prompt is designed to enhance accuracy by:\n",
        "\n",
        "- Focusing on Formula Identification: By asking the model to state the formula, it ensures that the correct method is chosen before solving, leading to more accurate answers.\n",
        "\n",
        "- Adding a Double-Check Step: The prompt instructs the model to verify its solution, which encourages careful calculation and helps catch errors, further improving reliability."
      ],
      "metadata": {
        "id": "HvW7VTADhLa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What are the merits and demerits of using advanced prompting approaches like Chain of Thought or Self-Consistency? (3 pts)\n",
        "\n",
        "Merits:\n",
        "\n",
        "- Higher Accuracy: Techniques like CoT improve problem-solving accuracy by guiding the model through reasoning steps.\n",
        "- Consistency: Self-Consistency uses multiple responses to reduce random errors and improve reliability.\n",
        "- Interpretability: Advanced prompts make the model’s reasoning clearer, which is helpful for step-by-step explanations.\n",
        "\n",
        "Demerits:\n",
        "\n",
        "- Costly: Longer prompts may hit token limits and increase request costs.\n",
        "- Higher Computation Cost: More computation due to multiple runs or lengthy prompts.\n",
        "- Prompt Dependency: Models may overfit to complex prompts, reducing adaptability."
      ],
      "metadata": {
        "id": "oSknWcWrhedg"
      }
    }
  ]
}