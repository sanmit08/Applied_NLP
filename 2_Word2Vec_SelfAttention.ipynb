{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "xwiNB_6Rx5M7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1.** Autograd for MLPs from scratch (20 pts)\n",
        "\n",
        "Automatic differentiation is the backbone of modern deep learning toolkits like Torch, JAX, TensorFlow, and MLX. To build an understanding of how these tools work, we will define functions and their gradients/Jacobians and compose them together to as the building blocks of a neural network\n",
        "\n",
        "The gradient of a real-valued function $f:\\mathbb{R}^n\\to\\mathbb{R}$ is a vector where\n",
        "$$\\nabla f(\\boldsymbol{x})_{i}=\\frac{\\partial f(\\boldsymbol{x})}{\\partial x_i}$$\n",
        "\n",
        "The Jacobian is a generalization of the gradient for a vector-valued function $f:\\mathbb{R}^m\\to\\mathbb{R}^n$. It is a $m\\times n$ matrix where\n",
        "$$J_f(\\boldsymbol{x})_{i,j}=\\frac{\\partial f(\\boldsymbol{x})_i}{\\partial x_j}$$\n",
        "\n",
        "Neural nets are generally built from composed functions. To calculate the Jacobian of composed functions, we can use matrix multiplication to apply the chain rule.\n",
        "\n",
        "$$\n",
        "J_{f\\circ g}(\\boldsymbol{x})\n",
        "= J_f(g(\\boldsymbol{x}))J_g(\\boldsymbol{x})\n",
        "$$\n",
        "\n",
        "([Proof](https://math.stackexchange.com/questions/3260609/how-to-show-jacobian-of-a-composite-function-is-the-product-of-jacobians).)\n",
        "\n",
        "We will define a series of functions which take 2D inputs and return 2D outputs.\n",
        "```\n",
        "arg1: (M, N)\n",
        "arg2: (O, P)\n",
        "...\n",
        "returns: (Q, R)\n",
        "```\n",
        "We will also define Jacobian functions, which return the function output as well as the Jacobian vector of each of the inputs.\n",
        "```\n",
        "arg1: (M, N)\n",
        "arg2: (O, P)\n",
        "...\n",
        "returns: (Q, R), (Q, R, M, N), (Q, R, O, P), ...\n",
        "```\n",
        "Note that the first two dimensions of the Jacobians match the output shape (Q, R), and the last two match the dimensions of the arguments so that they can be composed together\n",
        "\n",
        "```python\n",
        "output1, jac1 = func1_and_jac(x) # Shape: (M, N, O, P)\n",
        "output2, jac2 = func2_and_jac(output1) # Shape: (O, P, Q, R)\n",
        "jac = np.einsum(\"mnop,opqr->mnqr\", jac1, jac2) # Shape (M, N, Q, R)\n",
        "```"
      ],
      "metadata": {
        "id": "QS-G6o0OIo0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as np\n",
        "from numpy import testing\n",
        "import math\n",
        "\n",
        "def test(func, func_and_jac, *params):\n",
        "  testing.assert_allclose(func(*params), func_and_jac(*params)[0])\n",
        "  for i in range(len(params)):\n",
        "    print(f\"Testing Jacobian of parameter {i}\")\n",
        "    testing.assert_allclose(jax.jacfwd(func, argnums=i)(*params), func_and_jac(*params)[i + 1])\n",
        "  print(\"All passed!\")"
      ],
      "metadata": {
        "id": "lr0x4JJGXn9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference implementation of a func_and_jac function.\n",
        "def add_and_jac(x, y):\n",
        "  output = x + y\n",
        "  x_jac = np.eye(math.prod(x.shape)).reshape(*x.shape, *x.shape)\n",
        "  x_jac = np.broadcast_to(x_jac, (*output.shape, *x.shape))\n",
        "  y_jac = np.eye(math.prod(y.shape)).reshape(*y.shape, *y.shape)\n",
        "  y_jac = np.broadcast_to(y_jac, (*output.shape, *y.shape))\n",
        "  return output, x_jac, y_jac\n",
        "\n",
        "x, y = np.arange(24).reshape(2, 3, 4).astype(float)\n",
        "test(np.add, add_and_jac, x, y)"
      ],
      "metadata": {
        "id": "ABgqxvY-p0J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.1 (5 pts)\n",
        "Now its your turn: define the Jacobian of the matmul function. You can use `jax.jacfwd` to help check your work, but you should only use `numpy` functions in your solution."
      ],
      "metadata": {
        "id": "PiL_MwUGst78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def matmul_and_jac(x, y):\n",
        "  # TODO: define the Jacobian for the matmul function.\n",
        "  ### BEGIN IMPLEMENTATION ###\n",
        "  pass\n",
        "\n",
        "x = np.arange(12).reshape(3, 4).astype(float)\n",
        "y = np.arange(20).reshape(4, 5).astype(float)\n",
        "test(np.matmul, matmul_and_jac, x, y)"
      ],
      "metadata": {
        "id": "twyuvpUBp4NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.2 (5 pts)\n",
        "\n",
        "Now we will define a function that combine `matmul` and `add` to define the *affine* function (sometimes misleadingly called the \"linear\" function in neural network libraries)"
      ],
      "metadata": {
        "id": "fzME4-pEtU5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def affine(x, w, b):\n",
        "  return np.dot(x, w) + b\n",
        "\n",
        "def affine_and_jac(x, w, b):\n",
        "  # TODO: use add_and_jac and matmul_and_jac\n",
        "  # to define the Jacobian of the affine function\n",
        "  ### BEGIN IMPLEMENTATION ###\n",
        "  pass\n",
        "\n",
        "x = np.arange(12).reshape(3, 4).astype(float)\n",
        "w = np.arange(20).reshape(4, 5).astype(float)\n",
        "b = np.arange(5).reshape(1, 5).astype(float)\n",
        "test(affine, affine_and_jac, x, w, b)"
      ],
      "metadata": {
        "id": "atHiRVpDp6qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.3 (5 pts)\n",
        "Now lets define the Jacobian of some nonlinear functions (we'll implemnt `relu`, you implement `logsoftmax`.)"
      ],
      "metadata": {
        "id": "HTLbNskOtzCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  return np.maximum(x, 0)\n",
        "\n",
        "def relu_and_jac(x):\n",
        "  # Some libraries define $d relu(0) / dx$ to be 0.5.\n",
        "  x_jac = (np.diagflat(x > 0) + np.diagflat(x == 0) / 2).reshape(*x.shape, *x.shape)\n",
        "  return np.maximum(x, 0), x_jac\n",
        "\n",
        "def logsoftmax(x):\n",
        "  stabilized_x = x - np.max(x, axis=1, keepdims=True)\n",
        "  return stabilized_x - np.log(np.sum(np.exp(stabilized_x), axis=1, keepdims=True))\n",
        "\n",
        "def logsoftmax_and_jac(x):\n",
        "  # TODO: define the Jacobian of the log_softmax function\n",
        "  ### BEGIN IMPLEMENTATION ###\n",
        "  pass\n",
        "\n",
        "x = np.arange(12).reshape(3, 4).astype(float)\n",
        "test(logsoftmax, logsoftmax_and_jac, x)"
      ],
      "metadata": {
        "id": "ScKwmMiurkp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have all the building blocks, we can compose them together to create an MLP and its Jacobian!"
      ],
      "metadata": {
        "id": "pmLjPdUArqof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some utilities to deal with nested parameters and composing the Jacobians we defined.\n",
        "def tree_map(func, *trees: np.ndarray | tuple[np.ndarray], **kwargs):\n",
        "  if not isinstance(trees[0], tuple):\n",
        "    return func(*trees, **kwargs)\n",
        "  else:\n",
        "    return tuple(tree_map(func, *subtrees, **kwargs) for subtrees in zip(*trees))\n",
        "\n",
        "def sequential(func_and_jac_seq):\n",
        "  def func_and_jac(x, params_seq):\n",
        "    params_jac = ()\n",
        "    x, x_jac, _ = add_and_jac(x, np.zeros_like(x))\n",
        "    for func_and_jac, params in zip(func_and_jac_seq, params_seq):\n",
        "      x, x_layer_jac, *param_layer_jacs = func_and_jac(x, *params)\n",
        "      params_jac = tree_map(lambda param: np.einsum(\"...kl,klmn->...mn\", x_layer_jac, param), params_jac)\n",
        "      x_jac = np.einsum(\"...kl,klmn->...mn\", x_layer_jac, x_jac)\n",
        "      params_jac += (tuple(param_layer_jacs),)\n",
        "    return x, x_jac, params_jac\n",
        "  return func_and_jac\n",
        "\n",
        "# Here is a 2-layer MLP\n",
        "def mlp(x, params):\n",
        "  affine1_params, _, affine2_params, _ = params\n",
        "  return logsoftmax(affine(relu(affine(x, *affine1_params)), *affine2_params))\n",
        "\n",
        "# ...and its Jacobian!\n",
        "mlp_and_jac = sequential(\n",
        "    [affine_and_jac, relu_and_jac, affine_and_jac, logsoftmax_and_jac]\n",
        ")\n",
        "\n",
        "import numpy\n",
        "\n",
        "mlp_params = (\n",
        "    (\n",
        "      numpy.random.randn(10,100), # first matmul\n",
        "      numpy.random.randn(1, 100) # first bias\n",
        "    ),\n",
        "    (), # relu has no params\n",
        "    (\n",
        "      numpy.random.randn(100,10), # final matmul\n",
        "      numpy.random.randn(1, 10) # final bias\n",
        "    ),\n",
        "    (), # logsoftmax has no params\n",
        ")\n",
        "\n",
        "x = numpy.random.randn(3,10)\n",
        "y = np.eye(10)[numpy.random.randint(0, 10, size=3)]\n",
        "print(\"Testing MLP Jacobian\")\n",
        "tree_map(\n",
        "  numpy.testing.assert_allclose,\n",
        "  jax.jacfwd(mlp, argnums=1)(x, mlp_params),\n",
        "  mlp_and_jac(x, mlp_params)[2],\n",
        "  atol=1e-5\n",
        ")\n",
        "print(\"Passed!\")"
      ],
      "metadata": {
        "id": "6-MrO-19rt0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.4 (5 pts)\n",
        "Given the MLP Jacobian, if we define a loss function and its gradient, we can compose them together to get the parameter gradients of the MLP."
      ],
      "metadata": {
        "id": "8QJUAhh0sInG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(params, x, y):\n",
        "  logprobs = mlp(x, params)\n",
        "  return -np.mean(np.sum(logprobs * y, axis=1))\n",
        "\n",
        "def cross_entropy_loss_and_grad(logprobs, y):\n",
        "    return -np.mean(np.sum(logprobs * y, axis=1)), -y / logprobs.shape[0]\n",
        "\n",
        "x = numpy.random.randn(3,10)\n",
        "y = np.eye(10)[numpy.random.randint(0, 10, size=3)]\n",
        "logprobs, x_jac, params_jac = mlp_and_jac(x, mlp_params)\n",
        "loss, output_grad = cross_entropy_loss_and_grad(logprobs, y)\n",
        "\n",
        "param_grads = tree_map(\n",
        "    # TODO: Write the function to compute the parameter gradients\n",
        "    # using the output_grad and each parameter jacobian.\n",
        "    # BEGIN IMPLEMENTATION\n",
        "    ...,\n",
        "    params_jac\n",
        ")\n",
        "\n",
        "print(\"Testing MLP grad\")\n",
        "jax_grads = jax.grad(cross_entropy_loss)(mlp_params, x, y)\n",
        "tree_map(numpy.testing.assert_allclose, param_grads, jax_grads, atol=1e-5)\n",
        "print(\"All tests passed!\")"
      ],
      "metadata": {
        "id": "CupWwxYyr8x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have a taste for how autodiff works, take a look at this example, where we use JAX's autograd to do gradient descent, training an MLP to mimic a the sine function."
      ],
      "metadata": {
        "id": "wfb-MVQRsa0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nothing to do in this cell, just a reference implementation.\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import random, grad, jit\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Define the MLP Model\n",
        "def init_mlp_params(layer_sizes, key):\n",
        "    \"\"\"Initialize the parameters for an MLP.\"\"\"\n",
        "    params = []\n",
        "    keys = random.split(key, len(layer_sizes) - 1)\n",
        "    for in_size, out_size, key in zip(layer_sizes[:-1], layer_sizes[1:], keys):\n",
        "        weight_key, bias_key = random.split(key)\n",
        "        W = random.normal(weight_key, (in_size, out_size)) * jnp.sqrt(2.0 / in_size)\n",
        "        b = jnp.zeros(out_size)\n",
        "        params.append((W, b))\n",
        "    return params\n",
        "\n",
        "def mlp(params, x):\n",
        "    \"\"\"Forward pass for an MLP.\"\"\"\n",
        "    activations = x\n",
        "    for W, b in params[:-1]:\n",
        "        activations = jax.nn.relu(jnp.dot(activations, W) + b)\n",
        "    final_W, final_b = params[-1]\n",
        "    return jnp.dot(activations, final_W) + final_b\n",
        "\n",
        "# 2. Define the loss function\n",
        "def loss_fn(params, x, y):\n",
        "    predictions = mlp(params, x)\n",
        "    return jnp.mean((predictions - y) ** 2)\n",
        "\n",
        "# 3. Generate data (sine function between -π and π)\n",
        "key = random.PRNGKey(0)\n",
        "x_train = jnp.linspace(-jnp.pi, jnp.pi, 256).reshape(-1, 1)\n",
        "y_train = jnp.sin(x_train)\n",
        "\n",
        "# 4. Initialize the model parameters\n",
        "layer_sizes = [1, 64, 64, 1]  # MLP with two hidden layers\n",
        "params = init_mlp_params(layer_sizes, key)\n",
        "\n",
        "# 5. Gradient Descent update rule\n",
        "learning_rate = 0.01  # Set a suitable learning rate\n",
        "\n",
        "@jit\n",
        "def update(params, x, y):\n",
        "    \"\"\"Performs one update step using plain gradient descent.\"\"\"\n",
        "    grads = grad(loss_fn)(params, x, y)  # Compute gradients\n",
        "    new_params = [(W - learning_rate * dW, b - learning_rate * db)\n",
        "                  for (W, b), (dW, db) in zip(params, grads)]\n",
        "    return new_params\n",
        "\n",
        "# 6. Train the model\n",
        "num_epochs = 10000\n",
        "for epoch in range(num_epochs):\n",
        "    params = update(params, x_train, y_train)\n",
        "    if epoch % 1000 == 0:\n",
        "        loss = loss_fn(params, x_train, y_train)\n",
        "        print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "# 7. Plot the results\n",
        "x_test = jnp.linspace(-jnp.pi, jnp.pi, 256).reshape(-1, 1)\n",
        "y_pred = mlp(params, x_test)\n",
        "\n",
        "plt.plot(x_train, y_train, label='True function (sin)')\n",
        "plt.plot(x_test, y_pred, label='MLP approximation', linestyle='dashed')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z_7BuXtkseTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Co-occurrence Embeddings (20 points)\n",
        "Consider the technique of embedding words to vectors using counts of context co-occurrences, as taught in class.\n",
        "\n",
        "Here we will implement this technique to find embeddings for each unique word in the Shakespeare corpus. Later in the assignment, we will compare them to word embeddings learnt using Word2vec.\n"
      ],
      "metadata": {
        "id": "jHUtRC4Cx4dc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Q2.1 Co-occurrence + PPMI (15 points)\n",
        "\n",
        "(Full credit if close enough to the asserted value)\n",
        "\n",
        "Fill in the code to complete the implementation of calculating co-occurrences and also weighing them by PPMI (Positive Pointwise Mutual Infomation) as taught in class. All assert statements should pass. Use a span length of `5` words on each side (total window length 11) as already set default in the starter code provided.\n",
        "\n",
        "Note: Use log base e in your PPMI formula as mentioned in class slides."
      ],
      "metadata": {
        "id": "PTNuAnESrkJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data - do not change\n",
        "import requests\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import time\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status() # Raise an exception for invalid HTTP status codes\n",
        "text_data = response.text\n",
        "len(text_data), text_data[:100]\n"
      ],
      "metadata": {
        "id": "ehw3TlEF4oWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# preprocessing - do not change\n",
        "text_data = response.text\n",
        "text_data = text_data.replace(',',' , ').replace('.',' . ').replace('?',' ? ').replace('!',' ! ')\n",
        "text_data = text_data.replace('  ', ' ')\n",
        "text_data = text_data.replace('\\n\\n','\\n').replace('\\n',' </s> <s> ')\n",
        "text_data = '<s> ' + text_data + ' </s>'\n",
        "len(text_data), text_data[:100]"
      ],
      "metadata": {
        "id": "KQCd1lgS5raE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = text_data.split(' ')\n",
        "vocab = list(set(data))\n",
        "revmap = {v:k for k,v in enumerate(vocab)} # dictionary to map words to indices\n"
      ],
      "metadata": {
        "id": "GAQCKh8v4rN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = np.zeros(shape=(len(vocab), len(vocab)))\n",
        "assert counts.shape == (18118, 18118)\n",
        "\n",
        "span_length = 5\n",
        "start_time = time.time()\n",
        "\n",
        "# IMPLEMENT HERE\n",
        "\n",
        "for i,w in tqdm(enumerate(data), total=len(data)):\n",
        "  # Hint: calculate co-occurrences in each window\n",
        "\n",
        "# Hint: calculate PPMI values using co-occurrences\n",
        "ppmi = ...\n",
        "\n",
        "# Hint: Find L2 norms to convinently calculate cosine similiarity scores\n",
        "ppmi_norms = ...\n",
        "\n",
        "### END OF IMPLEMENTATION ####\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "# Note down this Training Time\n",
        "\n",
        "assert ppmi.shape == (len(vocab), len(vocab))\n",
        "assert counts[revmap['First'],revmap['Citizen:']] == 44.\n",
        "np.testing.assert_allclose(ppmi[revmap['First'],revmap['Citizen:']], 4.024266, rtol=1e-3)"
      ],
      "metadata": {
        "id": "Fo9XV5LX44LC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.2 Nearest Neighbours (5 points)\n",
        "Implement the function `get_nearest_neighbours` which given a target word (e.g., `eye`) or a vector (e.g., `[0.123, 1.993, ...]` as well as a 2D array `vectors` and the corresponding list of words `vocab`, returns `top_k` words that are most similar to the target vector in the embedding space, as measured by cosine similarity. Note that the closest word to the vector of `eye` must be `eye`."
      ],
      "metadata": {
        "id": "4UTV3lFlZcvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nearest_neighbours(word_or_vector, vectors=ppmi, top_k=5, vocab=vocab,\n",
        "                           norms=ppmi_norms):\n",
        "  # IMPLEMENT HERE\n",
        "  ...\n",
        "  # Hint: L2 norms can be used for ease of calculating cosine similarity.\n",
        "  # Hint: avoid iterating over vocabulary and use np.dot matrix multiplication.\n",
        "  # Hint: sort by cosine similarity scores."
      ],
      "metadata": {
        "id": "fcnL7Yvfw0fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_nearest_neighbours('he')\n",
        "print(get_nearest_neighbours('he'))\n",
        "assert 'he' in get_nearest_neighbours('he')"
      ],
      "metadata": {
        "id": "XFmwDCAIMaYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will evaluate the embeddings on Analogy Task. You do not need to implement anything in this subsection.\n",
        "Do not be concerned if your output results in a low score. Hit@100 is a metric which checks whether the ground truth answer was among the top 100 predictions made by a model.\n",
        "\n",
        "**Do note down the time taken to run inference over the benchmark, and the final Hit@100 scores.**\n"
      ],
      "metadata": {
        "id": "n0UueucVoP_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and preprocessing data - do not change\n",
        "import requests\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "url = \"http://download.tensorflow.org/data/questions-words.txt\"\n",
        "response = requests.get(url)\n",
        "response.raise_for_status() # Raise an exception for invalid HTTP status codes\n",
        "analogy = response.text.split('\\n')\n",
        "\n",
        "# filtering out analogies that do not exist in our vocabulary\n",
        "task = []\n",
        "for line in tqdm(analogy, total=len(analogy)):\n",
        "  row = line.split(' ')\n",
        "  if len(row) != 4 or line.startswith(':'):\n",
        "    continue\n",
        "  if sum(_ in vocab for _ in row) == 4:\n",
        "    task.append(row)\n",
        "len(task), task[-1]"
      ],
      "metadata": {
        "id": "rBPggSzNdQRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference over Analogy Benchmark - do not change\n",
        "start_time = time.time()\n",
        "acc = 0.\n",
        "for a,b,c,d in tqdm(task[:50]): # Note down this Inference Time\n",
        "  d_ = get_nearest_neighbours(\n",
        "        word_or_vector=ppmi[revmap[a]]+ppmi[revmap[b]]-ppmi[revmap[c]],\n",
        "        top_k=100\n",
        "      )\n",
        "  if d in d_:\n",
        "    acc += 1\n",
        "print('inference time:', time.time() - start_time)\n",
        "acc/len(task)"
      ],
      "metadata": {
        "id": "tGZlm9OedSuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3.** Word2vec Gradient Derivation (10 points)\n",
        "\n",
        "Remember the negative sampling loss function for Word2vec from the [lecture](https://drive.google.com/file/d/1xN5FiqtutlFrhID64qDGA3DqaZMIZfMh/view). Given the target word embedding $w$, context word embedding $c_\\text{pos}$ from positive examples, and $K$ noise word embeddings $c_\\text{neg$_i$}$ from negative examples,\n",
        "\n",
        "$L = - [\\log \\sigma(w.c_\\text{pos})+\\Sigma_{i=1..K} \\log \\sigma(-w.c_\\text{neg$_i$})]$\n",
        "\n",
        "where $\\sigma$ is the sigmoid function."
      ],
      "metadata": {
        "id": "rV4ycjVinvac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Q3.1*** Derive the following gradients. (4 points)\n",
        "**Hint:** First show that $\\frac{\\partial \\sigma(x)}{\\partial x} = \\sigma(x) (1 - \\sigma(x))$, and then use this throughout your derivations. Also, if you're not comfortable with LaTeX, feel free to write your answer on a sheet of paper and upload an image here."
      ],
      "metadata": {
        "id": "it8gxqe7-3eU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\frac{∂ L}{∂ w}$\n",
        "\n",
        "$\\frac{∂ L}{∂ c_\\text{pos}}$\n",
        "\n",
        "$\\frac{∂ L}{∂ c_\\text{neg$_i$}}$"
      ],
      "metadata": {
        "id": "5vi98lOd-6Ae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Q3.2*** In your own words, describe intuitively how embeddings change when updated using SGD under the gradients above (no additional equations besides the gradients above required, description in words is enough). (3 points)"
      ],
      "metadata": {
        "id": "JVPB3aMX-rDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EEk_LubW-qxz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Q3.3*** Why is negative sampling required? What happens if we solely rely on positive examples? (3 points)"
      ],
      "metadata": {
        "id": "Uwj9em-j-esW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EVjjwMPZ-oZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. Learn Word2Vec Embeddings (20 points)\n",
        "\n",
        "Here we will convert the above equations into code and learn word2vec embeddings for each word. We will then compare these embeddings to those learnt using PPMI in Q1."
      ],
      "metadata": {
        "id": "g9xeasHf_jrv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4.1 Learning Word2vec (15 points)\n",
        "\n",
        "Implement the Word2Vec as taught in class. Use a span length of `5` words on each side (total window length 11), `50` dimensions per embeddings, `0.01` learning rate, `15` negative samples per target word, and `500_000` number of iterations or trainig updates, as already set default in the starter code provided.\n",
        "\n",
        "While calculating the loss explicitly is not needed to find gradient updates, we require you to calculate it so we can log it every 10K steps (code already provided - do not change). This will help track the loss as it goes down over training steps.\n",
        "\n",
        "Hint: Do not try to find real negatives for a given context, instead randomly sample 15 words from the vocabulary to use as negative samples, and given the size of the vocabulary, it is very unlikely to include a word among the 11 context words.\n",
        "\n",
        "**Updated Hint**: Also optionally do not try to find samples without replacement. You can use `np.random.randint` to instead sample with replacement, which is much more efficient hence brings down training time.\n",
        "\n"
      ],
      "metadata": {
        "id": "hLD0EtaU_0yy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.special\n",
        "import numpy as np\n",
        "\n",
        "D = 50 # embedding_dimensions\n",
        "w_vectors = np.random.rand(len(vocab), D) # target\n",
        "c_vectors = np.random.rand(len(vocab), D) # context and noise"
      ],
      "metadata": {
        "id": "lRUSk7HWv8cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "LR = 1e-2 # learning_rate\n",
        "K = 15 # num_negatives\n",
        "span_length = 5 # number of words before and after target in context\n",
        "num_iter = 500_000 # number of iterations or update steps\n",
        "\n",
        "losses = []\n",
        "for iter in tqdm(range(num_iter)): # # Note down this Training Time\n",
        "  # IMPLEMENT HERE\n",
        "  # Hint: Pick a random target word from data, then iterate over each positive word in its context (using span_length)\n",
        "  # Hint: Fetch target, context, and negatives from w_vectors and c_vectors.\n",
        "  # Hint: Calculate gradient updates as derived in Q2.\n",
        "  # Hint: You may use scipy.special.expit function to implement sigmoid.\n",
        "  # Hint: Apply gradient updates to parts of w_vectors and c_vectors properly\n",
        "  if iter%10_000 == 0:\n",
        "    print(sum(losses)/len(losses)) # This should go down as training proceeds\n",
        "    losses = [] # resetting log every 10K steps\n",
        "\n",
        "print('training time:', time.time()-start_time)"
      ],
      "metadata": {
        "id": "W_BwOXkqAA0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w_vectors_norms = np.linalg.norm(w_vectors, axis=1)\n",
        "c_vectors_norms = np.linalg.norm(c_vectors, axis=1)"
      ],
      "metadata": {
        "id": "Z20QH35mcy56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_nearest_neighbours('First', vectors=w_vectors, top_k=5, vocab=vocab,\n",
        "                       norms=w_vectors_norms)"
      ],
      "metadata": {
        "id": "3p914znskw4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_nearest_neighbours('First', vectors=c_vectors, top_k=5, vocab=vocab,\n",
        "                       norms=c_vectors_norms)"
      ],
      "metadata": {
        "id": "q8nfcDWXugRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation - do not change\n",
        "start_time = time.time()\n",
        "hit_at_k = 0.\n",
        "for a,b,c,d in tqdm(task): # Note down this Inference Time\n",
        "  d_ = get_nearest_neighbours(\n",
        "        word_or_vector=w_vectors[revmap[a]]+w_vectors[revmap[b]]-w_vectors[revmap[c]],\n",
        "        top_k=100,\n",
        "        vectors=w_vectors,\n",
        "        norms=w_vectors_norms,\n",
        "      )\n",
        "  # Note that we could have used c_vectors as well instead of w_vectors\n",
        "  if d in d_:\n",
        "    hit_at_k += 1\n",
        "hit_at_k/=len(task)\n",
        "print('inference time:', time.time() - start_time)\n",
        "print(hit_at_k)"
      ],
      "metadata": {
        "id": "x59eG1yioV9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4.2 Comparison of embeddings (3 points)\n",
        "\n",
        "Now that we have trained two variants of embeddings, we can compare their results in terms of:\n",
        "- Analogy Task Hit@100 scores\n",
        "- Training and Inference Speed (time taken)\n",
        "- Memory Requirement\n",
        "\n",
        "Fill the following evaluations (based on runtime of the main loops within each embedding method, and total memory required for the embedding arrays):\n"
      ],
      "metadata": {
        "id": "hGmHRUXWxJbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys # Hint: use sys.getsizeof() and convert to MBs\n",
        "\n",
        "# IMPLEMENT HERE"
      ],
      "metadata": {
        "id": "wj5YcZRomI8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\# COMPLETE THIS TABLE.\n",
        "\n",
        "Round each value up to two decimal points and use seconds for speed, MBs for memory, and % for Analogy Hit@100 scores.\n",
        "\n",
        "Method|Training Speed|Inference Speed|Memory|Analogy\n",
        "---|---|---|---|---\n",
        "PPMI|\n",
        "SGNS|"
      ],
      "metadata": {
        "id": "oGItsjf-s0_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4.3 Difference in Results (2 points)\n",
        "\n",
        "Could you hypothesize the cause of the differences in training speed, inference speed, memory, and analogy results between the two methods?\n",
        "\n",
        "**\\# INSERT ANSWER HERE** \\\\\n"
      ],
      "metadata": {
        "id": "E2NxDNdms_J8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. Context Aware Embeddings using Self-Attention (25 + 5 points)"
      ],
      "metadata": {
        "id": "gi4rGS71kTFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this last question, you will build upon all the implementation done from Q1-4 to build your own context-aware embeddings using Self-Attention mechanism.\n",
        "\n",
        "## Recap Self-Attention\n",
        "\n",
        "Recall that Self-Attention is a type of a scaled dot-product attention. It allows the model to weigh different positions of a sequence differently when constructing a representation of that sequence. This is crucial in NLP tasks because it enables the model to focus on relevant parts of a sentence, regardless of their position in the sequence.\n",
        "\n",
        "Given an input sequence with $n$ tokens, each token is first converted into an embedding vector. Let's denote these embeddings as $\\mathbf{X} \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$, where $d_{\\text{model}}$ is the dimension of the embeddings.\n",
        "\n",
        "For each token, we compute three vectors: Query ($\\mathbf{Q}$), Key ($\\mathbf{K}$), and Value ($\\mathbf{V}$). These vectors are obtained by multiplying the input embeddings by three learnable weight matrices:\n",
        "\n",
        "$$\n",
        "\\mathbf{Q} = \\mathbf{X}\\mathbf{W}_Q, \\quad \\mathbf{K} = \\mathbf{X}\\mathbf{W}_K, \\quad \\mathbf{V} = \\mathbf{X}\\mathbf{W}_V\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "*  $\\mathbf{W}_Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$,\n",
        "*  $\\mathbf{W}_K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$\n",
        "*  $\\mathbf{W}_V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$\n",
        "\n",
        "$d_k$ and $d_v$ are the dimensions of the Query/Key and Value vectors, respectively.\n",
        "\n",
        "The attention mechanism calculates the attention scores between the Query and Key vectors to determine how much focus should be placed on different tokens in the sequence.\n",
        "\n",
        "The attention scores are computed using the dot product of the Query and Key vectors, scaled by the square root of the dimension of the Key vectors $d_k$:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right)\\mathbf{V}\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "* $\\mathbf{Q}\\mathbf{K}^\\top$ gives the similarity scores between the Query and Key vectors.\n",
        "* $\\frac{1}{\\sqrt{d_k}}$ is a scaling factor to prevent the dot products from growing too large, which could lead to very small gradients.\n",
        "* The softmax function normalizes the scores so that they sum to 1.\n",
        "* The resulting matrix is then multiplied by the Value matrix $\\mathbf{V}$ to produce the output.\n",
        "\n"
      ],
      "metadata": {
        "id": "srk1IYYtkWuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question, we will use a PyTorch-style autograd (again implemented in NumPY) instead of the JAX-style autograd from Q1. Let's first define an abstract class that will be the base of all neural network components and a few basic components. Here, backpropagation relies on forward and backward passes to calculate the gradients required for updating parameters. So each component needs to have a `forward` and `backward` method."
      ],
      "metadata": {
        "id": "mqQsz7mvPWXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change\n",
        "import numpy as np\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class NNComp(ABC):\n",
        "    @abstractmethod\n",
        "    def forward(self, x):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, incoming_grad):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Linear(NNComp):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        k = 1 / in_dim\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "\n",
        "        # forward stuff, we need to initialize the weight and the bias\n",
        "        self.weight = np.random.uniform(-np.sqrt(k), np.sqrt(k), size=(in_dim, out_dim))\n",
        "        self.bias = np.random.uniform(-np.sqrt(k), np.sqrt(k), size=out_dim)\n",
        "\n",
        "        # backward stuff\n",
        "        self.backward_context = None\n",
        "        self.dweight = np.empty_like(self.weight, dtype=float)\n",
        "        self.dbias = np.empty_like(self.bias, dtype=float)\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert np.shape(x)[-1] == self.in_dim\n",
        "        self.backward_context = x\n",
        "        return x @ self.weight + self.bias\n",
        "\n",
        "    def backward(self, incoming_grad):\n",
        "        self.dweight = self.backward_context.T @ incoming_grad\n",
        "        self.dbias = incoming_grad.sum(axis=0)\n",
        "        return incoming_grad @ self.weight.T\n",
        "\n",
        "    def update_parameters(self, lr):\n",
        "        self.weight -= lr * self.dweight\n",
        "        self.bias -= lr * self.dbias\n",
        "\n",
        "\n",
        "class LogSoftmax(NNComp):\n",
        "    def __init__(self):\n",
        "        self.backward_context = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.backward_context = x - np.max(x, axis=-1, keepdims=True)\n",
        "        return self.backward_context - np.log(np.exp(self.backward_context).sum(axis=-1, keepdims=True))\n",
        "\n",
        "    def backward(self, incoming_grad):\n",
        "        softmax = np.exp(self.backward_context) / np.exp(self.backward_context).sum(axis=-1, keepdims=True)\n",
        "        return incoming_grad - softmax * incoming_grad.sum(axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "vhhy9Sdtwh0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Q5.1*** Implement the Self-Attention Module. (10 points)\n",
        "\n",
        "Based on the description of Linear and LogSoftmax classes shown above, implement the Self-Attention module below.\n",
        "\n",
        "__Hint__: Make use of broadcasting in numpy for operations to implement attention."
      ],
      "metadata": {
        "id": "4wOZPRtnP9jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "ZAUml-JelmhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(NNComp):\n",
        "  def __init__(self, embed_dim):\n",
        "    # do not change\n",
        "    self.embed_dim = embed_dim # Specify the embedding dimension\n",
        "\n",
        "    # Initializer Q, K, V vectors\n",
        "    self.query_layer = Linear(embed_dim, embed_dim)\n",
        "    self.key_layer = Linear(embed_dim, embed_dim)\n",
        "    self.value_layer = Linear(embed_dim, embed_dim)\n",
        "    self.softmax = LogSoftmax()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Start with setting up the Q, K, V vectors\n",
        "    # Compute the attention scores using Q, K and normalize them using softmax\n",
        "    # Combine the attention weights with V\n",
        "    # Don't forget to store the backward context\n",
        "    # IMPLEMENT HERE\n",
        "    ...\n",
        "\n",
        "  def backward(self, incoming_grad):\n",
        "    # Implement backward for Self-Attention, feel free to use the backward() functions defined for modules above\n",
        "    # Utilize the gradient formulation provided above\n",
        "    # IMPLEMENT HERE\n",
        "    ...\n",
        "\n",
        "  def update_parameters(self, lr):\n",
        "    # Implement update here\n",
        "    # IMPLEMENT HERE\n",
        "    ...\n"
      ],
      "metadata": {
        "id": "T0_n2A20kV_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Q5.2*** Training Self-Attention Module (10 points)\n",
        "\n",
        "Now, you will use the word embeddings developed in Q4 (word2vec) and apply Self-Attention on them to obtain **contextual embeddings**.\n",
        "\n",
        "Contextual embeddings, like those from BERT or GPT, capture the meaning of a word based on the context in which it appears, producing different embeddings for the same word in different contexts. In contrast, Word2Vec generates static embeddings, where each word has a single fixed vector representation regardless of its context.\n",
        "\n",
        "For this, we will explore a new dataset from the [SuperGLUE Benchmark](https://super.gluebenchmark.com/) called 'Words in Context' (WiC [link text](https://aclanthology.org/N19-1128/)). This task evaluates settings where the \"context\" of the word is important to infer its meaning -- for example, the meaning of the word \"bank\" changes when used in context of money or a river.\n",
        "\n",
        "This dataset comprises of a pair of sentences, and a binary label. The label is 1 if a target word is used in the same context, and 0 otherwise. See examples below."
      ],
      "metadata": {
        "id": "BEetHtyDRhKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the SuperGLUE WiC dataset\n",
        "dataset = load_dataset('super_glue', 'wic', trust_remote_code=True)\n",
        "\n",
        "# Extract the training and testing data\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['validation']\n",
        "\n",
        "# Prepare sentences and labels\n",
        "sentences_1_train = train_data['sentence1']\n",
        "sentences_2_train = train_data['sentence2']\n",
        "labels_train = train_data['label']\n",
        "\n",
        "sentences_1_test = test_data['sentence1']\n",
        "sentences_2_test = test_data['sentence2']\n",
        "labels_test = test_data['label']"
      ],
      "metadata": {
        "id": "CWAcFkkYD55D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of an instance where the meaning of the word \"place\" is not the same\n",
        "sentences_1_train[0], sentences_2_train[0], labels_train[0]"
      ],
      "metadata": {
        "id": "LFox7OcVWRBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of an instance where the meaning of the word \"summer\" is the same\n",
        "sentences_1_train[4], sentences_2_train[4], labels_train[4]"
      ],
      "metadata": {
        "id": "pGVa2FFGWsdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given this dataset, you first need to train a Self-Attention Model over the word2vec embeddings, to better learn the context and position of words on this dataset.\n",
        "\n",
        "For training self-attention, we will make use of a Binary Cross Entropy Loss. First, we perform a forward pass through the self-attention model for both sentences:\n",
        "\n",
        "$$\n",
        "\\text{output}_1 = \\text{AttentionModel}(\\text{emb}_1) \\quad \\text{and} \\quad \\text{output}_2 = \\text{AttentionModel}(\\text{emb}_2)\n",
        "$$\n",
        "\n",
        "These outputs are the contextual embeddings for the first and second sentences, respectively.\n",
        "\n",
        "Next, we compute the similarity score between the two outputs using cosine similarity:\n",
        "\n",
        "$$\n",
        "\\text{similarity} = \\frac{\\text{output}_1 \\cdot \\text{output}_2}{\\|\\text{output}_1\\| \\|\\text{output}_2\\|}\n",
        "$$\n",
        "\n",
        "We then apply the sigmoid function to this similarity score to obtain a predicted probability:\n",
        "\n",
        "$$\n",
        "\\text{predicted} = \\frac{1}{1 + e^{-\\text{similarity}}}\n",
        "$$\n",
        "\n",
        "Given a target label $\\text{target} \\in \\{0, 1\\}$, we compute the Binary Cross-Entropy Loss:\n",
        "\n",
        "$$\n",
        "\\text{loss} = - \\left( \\text{target} \\cdot \\log(\\text{predicted} + 10^{-10}) + (1 - \\text{target}) \\cdot \\log(1 - \\text{predicted} + 10^{-10}) \\right)\n",
        "$$\n",
        "\n",
        "Note that we add $10^{-10}$ for stability in log, during training.\n",
        "\n",
        "The gradient of the loss with respect to the predicted output is:\n",
        "\n",
        "$$\n",
        "\\text{grad_loss} = \\text{predicted} - \\text{target}\n",
        "$$\n",
        "\n",
        "Finally, we backpropagate the gradients through the attention model for both embeddings.\n"
      ],
      "metadata": {
        "id": "3oK7lh85XmjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change, helper functions to implement padding for w2v embeddings\n",
        "\n",
        "def normalize_embeddings(embeddings):\n",
        "  mean = np.mean(embeddings, axis=0)\n",
        "  std = np.std(embeddings, axis=0)\n",
        "  return (embeddings - mean) / (std + 1e-8)\n",
        "\n",
        "def extract_w2v_embeddings(sentences, max_tokens=10):\n",
        "  embeddings = []\n",
        "  for sentence in sentences:\n",
        "    w2v_embeddings_sentence = []\n",
        "    words = sentence.split()\n",
        "\n",
        "    for word in words:\n",
        "      if word in revmap:\n",
        "        w2v_embeddings_sentence.append(w_vectors[revmap[word]])\n",
        "\n",
        "    if len(w2v_embeddings_sentence) < max_tokens:\n",
        "      pad_tokens = max_tokens - len(w2v_embeddings_sentence)\n",
        "      w2v_embeddings_sentence.extend([np.zeros(D) for i in range(pad_tokens)])\n",
        "    else:\n",
        "      w2v_embeddings_sentence = w2v_embeddings_sentence[:max_tokens]\n",
        "\n",
        "    embeddings.append(np.array(w2v_embeddings_sentence))\n",
        "\n",
        "  return embeddings\n",
        "\n",
        "input_embeddings_1 = extract_w2v_embeddings(sentences_1_train)\n",
        "input_embeddings_2 = extract_w2v_embeddings(sentences_2_train)"
      ],
      "metadata": {
        "id": "3v2d4nOQf2FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example learning rate and number of epochs\n",
        "lr = 1e-5\n",
        "epochs = 10\n",
        "\n",
        "attention_model = SelfAttention(D)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  total_loss = 0\n",
        "  for i, (emb1, emb2) in enumerate(zip(input_embeddings_1, input_embeddings_2)):\n",
        "    # IMPLEMENT HERE\n",
        "    # Forward pass through self-attention model for both sentences\n",
        "\n",
        "    # Compute similarity score between the two outputs (e.g., cosine similarity)\n",
        "\n",
        "    # Define target (either 0 or 1)\n",
        "\n",
        "    # Compute Binary Cross-Entropy Loss\n",
        "\n",
        "    # Compute gradient of the loss w.r.t. predicted output\n",
        "\n",
        "    # Backpropagate the gradients through the attention model for both embeddings\n",
        "\n",
        "    # Update parameters\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(input_embeddings_1)}\")"
      ],
      "metadata": {
        "id": "9dSYlRqJbCjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Q5.3*** Word2vec vs. Contextual Embeddings (5 points)"
      ],
      "metadata": {
        "id": "GqZ-uHYQwvKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now compare word2vec embeddings and the contextual embeddings we just made on the WiC task from above. Please make a note of the final accuracies.\n",
        "\n",
        "Nothing to code here, just note the final accuracies."
      ],
      "metadata": {
        "id": "Q3ytiioEQD4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change\n",
        "\n",
        "# get_sentence_embeddings is a general purpose function that will create sentence embeddings given any type of embeddings\n",
        "def get_sentence_embeddings(sentences, attention_model=None):\n",
        "  feature_matrix = np.zeros((len(sentences), D))\n",
        "\n",
        "  for i, sentence in enumerate(sentences):\n",
        "    words = sentence.split()\n",
        "\n",
        "    # We only look for words that occur in our vocabulary (specially since we are using a different dataset to build the vocabulary)\n",
        "    word_indices = [revmap[word] for word in words if word in revmap]\n",
        "\n",
        "    # Sentence embeddings are created by aggregating the word embeddings obtained\n",
        "    if word_indices:\n",
        "      word_embeddings = w_vectors[word_indices, :]\n",
        "      if attention_model:\n",
        "        word_embeddings = attention_model.forward(word_embeddings)\n",
        "      feature_matrix[i] = np.mean(word_embeddings, axis=0)\n",
        "    else:\n",
        "      feature_matrix[i] = np.zeros(D)\n",
        "\n",
        "  return feature_matrix"
      ],
      "metadata": {
        "id": "cNoEoc6ZKbXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change\n",
        "\n",
        "# Extract features for both sets of sentences in training and testing\n",
        "features_1_train_word2vec = get_sentence_embeddings(sentences_1_train)\n",
        "features_2_train_word2vec = get_sentence_embeddings(sentences_2_train)\n",
        "features_1_test_word2vec = get_sentence_embeddings(sentences_1_test)\n",
        "features_2_test_word2vec = get_sentence_embeddings(sentences_2_test)\n",
        "\n",
        "# Same for contextual embeddings\n",
        "features_1_train_contextual = get_sentence_embeddings(sentences_1_train, attention_model=attention_model)\n",
        "features_2_train_contextual = get_sentence_embeddings(sentences_2_train, attention_model=attention_model)\n",
        "features_1_test_contextual = get_sentence_embeddings(sentences_1_test, attention_model=attention_model)\n",
        "features_2_test_contextual = get_sentence_embeddings(sentences_2_test, attention_model=attention_model)"
      ],
      "metadata": {
        "id": "xkkPsKpTlY5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Combine the features from both sentences for the classifier input\n",
        "X_train_word2vec = np.concatenate([features_1_train_word2vec, features_2_train_word2vec], axis=1)\n",
        "X_test_word2vec = np.concatenate([features_1_test_word2vec, features_2_test_word2vec], axis=1)\n",
        "\n",
        "X_train_contextual = np.concatenate([features_1_train_contextual, features_2_train_contextual], axis=1)\n",
        "X_test_contextual = np.concatenate([features_1_test_contextual, features_2_test_contextual], axis=1)"
      ],
      "metadata": {
        "id": "__4N64hJKA-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do not change\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Train classifier on word2vec embeddings\n",
        "clf_word2vec = LogisticRegression(max_iter=1000)\n",
        "clf_word2vec.fit(X_train_word2vec, labels_train)\n",
        "pred_word2vec = clf_word2vec.predict(X_test_word2vec)\n",
        "\n",
        "# Train classifier on contextual embeddings\n",
        "clf_contextual = LogisticRegression(max_iter=1000)\n",
        "clf_contextual.fit(X_train_contextual, labels_train)\n",
        "pred_contextual = clf_contextual.predict(X_test_contextual)\n",
        "\n",
        "# Evaluate and compare accuracies\n",
        "accuracy_word2vec = accuracy_score(labels_test, pred_word2vec)\n",
        "accuracy_contextual = accuracy_score(labels_test, pred_contextual)\n",
        "\n",
        "print(f\"Word2Vec Embeddings Accuracy: {accuracy_word2vec}\")\n",
        "print(f\"Contextual Embeddings Accuracy: {accuracy_contextual}\")"
      ],
      "metadata": {
        "id": "Fdx6SXXaKog0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Q5.4*** (Bonus) Word2vec vs. Contextual Embeddings (5 points)\n",
        "\n",
        "What do you observe above from accuracy values, for both contextual and word2vec embeddings? Which one performs better and why?"
      ],
      "metadata": {
        "id": "jAhY3Q983sUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**[INSERT YOUR ANSWER HERE]**"
      ],
      "metadata": {
        "id": "mWrvGoGb368S"
      }
    }
  ]
}