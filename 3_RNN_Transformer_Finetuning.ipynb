{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1 - RNNs\n",
        "\n",
        "In this section, you will implement a simple Recurrent Neural Network (RNN) from scratch to perform sequence prediction using the IMDb movie reviews dataset.\n",
        "\n",
        "![RNN.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "kVCKsKZRQUqD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing Libraries\n",
        "\n",
        "First we import the necessary libraries for data manipulation, model building, and training. No other libraries are allowed to be imported."
      ],
      "metadata": {
        "id": "j71WWCtN0Mci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChCCwjXNZL_m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from gensim.models import FastText, KeyedVectors\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load and Prepare Data\n",
        "\n",
        "Next, we load the IMDb dataset and prepare it for training."
      ],
      "metadata": {
        "id": "EXz0kJe_0WEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "maxlen = 100"
      ],
      "metadata": {
        "id": "3gSq5PBFaOXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = imdb.load_data(num_words=vocab_size + 3, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "RLL67eJtaPMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a one-hot encoded representation of the training data\n",
        "x_train_one_hot_encoded = np.zeros((x_train.shape[0], maxlen, vocab_size))\n",
        "\n",
        "# Iterate over each sample and each word index to set the one-hot value to 1\n",
        "for i in range(x_train.shape[0]):\n",
        "    for j in range(len(x_train[i])):\n",
        "        idx = x_train[i][j] - 3\n",
        "        if idx < 0:\n",
        "          idx = 0\n",
        "        x_train_one_hot_encoded[i, j, idx] = 1"
      ],
      "metadata": {
        "id": "mycpHxZP0RW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode the dataset back into words to train Word2Vec\n",
        "word_index = imdb.get_word_index()\n",
        "index_word = {v: k for k, v in word_index.items() if v < vocab_size}\n",
        "\n",
        "x_train_words = [[index_word.get(idx-3, \"unk\") for idx in sequence] for sequence in x_train]\n",
        "x_test_words = [[index_word.get(idx-3, \"unk\") for idx in sequence] for sequence in x_test]"
      ],
      "metadata": {
        "id": "lwxWAb9TaQI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = api.load(\"glove-twitter-50\")\n",
        "embedding_size = embedding_model.vector_size"
      ],
      "metadata": {
        "id": "HtlBNYZqPVKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_sequences(sequences, embedding_model, maxlen, embedding_size):\n",
        "    embeddings = []\n",
        "    for sequence in sequences:\n",
        "        seq_embedding = [\n",
        "            embedding_model[word] if word in embedding_model else np.zeros(embedding_size)\n",
        "            for word in sequence\n",
        "        ]\n",
        "        if len(seq_embedding) < maxlen:\n",
        "            # Padding with zero vectors if sequence is shorter\n",
        "            seq_embedding += [np.zeros(embedding_size)] * (maxlen - len(seq_embedding))\n",
        "        if len(seq_embedding) > maxlen:\n",
        "            seq_embedding = seq_embedding[:maxlen]\n",
        "\n",
        "        embeddings.append(seq_embedding)\n",
        "    return np.array(embeddings)"
      ],
      "metadata": {
        "id": "LhTZB87gaRML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_embeddings = embed_sequences(x_train_words, embedding_model, maxlen, embedding_size)\n",
        "y_train_outputs = x_train_one_hot_encoded\n",
        "\n",
        "# Get correct values for next word prediction\n",
        "x_train_embeddings = x_train_embeddings[:, :maxlen - 1, :]\n",
        "y_train_outputs = y_train_outputs[:, -(maxlen - 1):, :]"
      ],
      "metadata": {
        "id": "UyCuVKguaTMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Softmax Function (2 Points)\n",
        "\n",
        "Implement the softmax function to be used in the output layer. The fucntion should take in a one dimensional numpy array and compute softmax."
      ],
      "metadata": {
        "id": "EBXa5NZ60uuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x: np.ndarray) -> np.ndarray:\n",
        "    ### BEGIN IMPLEMENTATION ###\n",
        "    pass"
      ],
      "metadata": {
        "id": "FnRfxzrNkR2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input Layer Class (8 points)\n",
        "\n",
        "Define the InputLayer class. The input player includes the following variables:   This variable represents the weight matrix connecting the input layer to the hidden layer. The dimensions are defined by the hidden size and the size of the input at each time step. It is initialized with small random values to facilitate learning during training.\n",
        "\n",
        "Variables:\n",
        "- `inputs`: The input sequences for the RNN, which are encoded using word2vec. It has a shape of (max_sequence_length, word2vec_size).\n",
        "- `weights`: This weight matrix connecting the input layer to the hidden layer.\n",
        "- `delta_weights`: The accumulation of the weight matrix gradients calculated across timesteps during backpropogation.\n",
        "\n",
        "You must implement the following functions:\n",
        "- `forward`: Multiply the input by the weight matrix. The output is a weighted sum that will be passed to the hidden layer for further processing. (4 points)\n",
        "\n",
        "- `backward`: Calculate the gradient of the forward operation and add that value to `self.delta_weights`, to be updated in `update_parameters`. (4 points)\n",
        "\n"
      ],
      "metadata": {
        "id": "9-5iuTIu4ucG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InputLayer:\n",
        "    inputs: np.ndarray\n",
        "    weights: np.ndarray = None\n",
        "    delta_weights: np.ndarray = None\n",
        "\n",
        "    def __init__(self, inputs: np.ndarray, hidden_size: int) -> None:\n",
        "        self.inputs = inputs\n",
        "\n",
        "        limit = np.sqrt(6 / (len(inputs[0]) + hidden_size))\n",
        "        self.weights = np.random.uniform(low=-limit, high=limit, size=(hidden_size, len(inputs[0])))\n",
        "\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def __reset_deltas__(self):\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def get_input(self, time_step: int) -> np.ndarray:\n",
        "        return self.inputs[time_step][:, np.newaxis]\n",
        "\n",
        "    def forward(self, time_step: int) -> np.ndarray:\n",
        "        ### BEGIN IMPLEMENTATION ###\n",
        "        return self.weights @ self.get_input(time_step)\n",
        "        ### END IMPLEMENTATION ###\n",
        "\n",
        "    def backward(\n",
        "        self, time_step: int, delta_weights: np.ndarray\n",
        "    ) -> None:\n",
        "        ### BEGIN IMPLEMENTATION ###\n",
        "        pass\n",
        "\n",
        "    def update_parameters(self, learning_rate: float) -> None:\n",
        "        self.weights -= learning_rate * self.delta_weights\n",
        "\n",
        "        self.__reset_deltas__()"
      ],
      "metadata": {
        "id": "zRgpALOwaUm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hidden Layer Class (8 points)\n",
        "\n",
        "Define the HiddenLayer class. The hidden layer maintains the hidden states across time steps and computes the activations based on the weighted sum of the inputs, computed in the input layer, and the previous hidden state.\n",
        "\n",
        "variables:\n",
        "- `states`: The hidden states for all time steps during the sequence processing. It has a shape of (max_num_time_steps, hidden_size, 1).\n",
        "\n",
        "- `weights`: The weight matrix connecting the previous hidden layer.\n",
        "\n",
        "- `bias`: The bias vector added to the weighted sum of inputs and hidden states.\n",
        "\n",
        "- `delta_weights`: The accumulation of gradients of the weight matrix computed during backpropogation.\n",
        "\n",
        "- `delta_bias`: The accumulation of gradients of the bias vector computed during backpropogation.\n",
        "\n",
        "- `next_delta_hidden_state_activation`: The gradient of the previously calculated hidden state activation.\n",
        "\n",
        "\n",
        "You must implement the following functions:\n",
        "\n",
        "- `forward`: This function computes the forward pass through the hidden layer. It combines the weighted input, the previous hidden state, and the bias, followed by applying the hyperbolic tangent activation function (tanh). The resulting activation is stored and returned for further processing. Be sure to update the hidden state using `set_hidden_state`. (4 points)\n",
        "\n",
        "- `backward`: This function computes the gradients for backpropagation. It calculates the delta for the activation using the incoming delta_output, propagates the gradients back to the previous hidden state, and updates delta_weights and delta_bias based on the computed gradients. (4 points)"
      ],
      "metadata": {
        "id": "UQFvMItcKi_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HiddenLayer:\n",
        "    states: np.ndarray = None\n",
        "    weights: np.ndarray = None\n",
        "    delta_weights: np.ndarray = None\n",
        "    bias: np.ndarray = None\n",
        "    delta_bias: np.ndarray = None\n",
        "    next_delta_hidden_state_activation: np.ndarray = None\n",
        "\n",
        "    def __init__(self, max_num_time_steps: int, size: int) -> None:\n",
        "        limit = np.sqrt(6 / (size + size))\n",
        "        self.weights = np.random.uniform(low=-limit, high=limit, size=(size, size))\n",
        "\n",
        "        self.bias = np.random.uniform(low=-0.1, high=0.1, size=(size, 1))\n",
        "        self.states = np.zeros(shape=(max_num_time_steps, size, 1))\n",
        "        self.next_delta_hidden_state_activation = np.zeros(shape=(size, 1))\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def __reset_deltas__(self):\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "        self.next_delta_hidden_state_activation = np.zeros_like(self.next_delta_hidden_state_activation)\n",
        "\n",
        "    def __reset_states__(self):\n",
        "        self.states = np.zeros_like(self.states)\n",
        "\n",
        "    def get_hidden_state(self, time_step: int) -> np.ndarray:\n",
        "            if time_step < 0:\n",
        "                return np.zeros_like(self.states[0])\n",
        "            return self.states[time_step]\n",
        "\n",
        "    def set_state(self, time_step: int, prediction: np.ndarray) -> None:\n",
        "        self.states[time_step] = prediction\n",
        "\n",
        "    def forward(self, weighted_input: np.ndarray, time_step: int) -> np.ndarray:\n",
        "        ### BEGIN IMPLEMENTATION ###\n",
        "        pass\n",
        "\n",
        "    def backward(\n",
        "        self, time_step: int, delta_output: np.ndarray\n",
        "    ) -> np.ndarray:\n",
        "        ### BEGIN IMPLEMENTATION ###\n",
        "        pass\n",
        "\n",
        "    def update_parameters(self, learning_rate: float) -> None:\n",
        "        self.weights -= learning_rate * self.delta_weights\n",
        "        self.bias -= learning_rate * self.delta_bias\n",
        "\n",
        "        self.__reset_deltas__()\n"
      ],
      "metadata": {
        "id": "mwTWFPu0au0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Output Layer Class (8 points)\n",
        "\n",
        "Define the OutputLayer class. The output layer will generate a probability distribution from the hidden state.\n",
        "\n",
        "Variables:\n",
        "- `states`: The computed probability distributions across all timesteps.\n",
        "\n",
        "- `weights`: The weight matrix connecting the hidden layer.\n",
        "\n",
        "- `bias`: The bias vector added to the output of the weighted sum.\n",
        "\n",
        "- `delta_weights`: The accumulation of gradients of the weight matrix computed during backpropogation.\n",
        "\n",
        "- `delta_bias`: The accumulation of gradients of the bias vector computed during backpropogation.\n",
        "\n",
        "You must implement the following functions:\n",
        "- `forward`: This function computes the forward pass through the output layer. It multiplies the hidden state by the weight matrix, adds the bias, and applies the softmax function to produce a probability distribution over the vocabulary. The resulting predictions MUST be stored using `set_prediction`. (4 points)\n",
        "\n",
        "- `backward`: This function computes the gradients for backpropagation based on the expected outputs, updates delta_V and delta_bias with the computed gradients, and returns the propagated error to the previous layer. (4 points)\n"
      ],
      "metadata": {
        "id": "PzZ0uRP5O-S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OutputLayer:\n",
        "    predictions: np.ndarray = None\n",
        "    weights: np.ndarray = None\n",
        "    bias: np.ndarray = None\n",
        "    delta_bias: np.ndarray = None\n",
        "    delta_weights: np.ndarray = None\n",
        "\n",
        "    def __init__(self, max_num_time_steps: int, size: int, hidden_size: int) -> None:\n",
        "        limit = np.sqrt(6 / (size + hidden_size))\n",
        "        self.weights = np.random.uniform(low=-limit, high=limit, size=(size, hidden_size))\n",
        "        self.bias = np.random.uniform(low=-0.1, high=0.1, size=(size, 1))\n",
        "        self.predictions = np.zeros(shape=(max_num_time_steps, size, 1))\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def __reset_predictions__(self):\n",
        "        self.predictions = np.zeros_like(self.predictions)\n",
        "\n",
        "    def __reset_deltas__(self):\n",
        "        self.delta_bias = np.zeros_like(self.bias)\n",
        "        self.delta_weights = np.zeros_like(self.weights)\n",
        "\n",
        "    def forward(self, hidden_state: np.ndarray, time_step: int) -> np.ndarray:\n",
        "        ### BEGIN IMPLEMENTATION ###\n",
        "        pass\n",
        "\n",
        "    def get_prediction(self, time_step: int) -> np.ndarray:\n",
        "        return self.predictions[time_step]\n",
        "\n",
        "    def set_prediction(self, time_step: int, prediction: np.ndarray) -> None:\n",
        "        self.predictions[time_step] = prediction\n",
        "\n",
        "    def backward(\n",
        "        self,\n",
        "        expected: np.ndarray,\n",
        "        hidden_state: np.ndarray,\n",
        "        time_step: int,\n",
        "    ) -> np.ndarray:\n",
        "        ### BEGIN IMPLEMENTATION ###\n",
        "        pass\n",
        "\n",
        "    def update_parameters(self, learning_rate: float) -> None:\n",
        "        self.weights -= learning_rate * self.delta_weights\n",
        "        self.bias -= learning_rate * self.delta_bias\n",
        "\n",
        "        self.__reset_deltas__()"
      ],
      "metadata": {
        "id": "0SfYV3ircDlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### RNN Class\n",
        "The RNN class is a Recurrent Neural Network that combines the input, hidden, and output layers to process sequences of inputs, learn their representations, and generate predictions."
      ],
      "metadata": {
        "id": "li17DVjCCYUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "class RNN:\n",
        "    hidden_layer: HiddenLayer\n",
        "    output_layer: OutputLayer\n",
        "    learning_rate: float\n",
        "    input_layer: InputLayer = None\n",
        "\n",
        "    def __init__(self, vocab_size: int, hidden_size: int, max_num_time_steps: int, learning_rate: float) -> None:\n",
        "        self.hidden_layer = HiddenLayer(max_num_time_steps, hidden_size)\n",
        "        self.output_layer = OutputLayer(max_num_time_steps, vocab_size, hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def feed_forward(self, inputs: np.ndarray) -> OutputLayer:\n",
        "        self.input_layer = InputLayer(inputs, self.hidden_size)\n",
        "\n",
        "        for step in range(len(inputs)):\n",
        "            weighted_input = self.input_layer.forward(step)\n",
        "            activation = self.hidden_layer.forward(weighted_input, step)\n",
        "            self.output_layer.forward(activation, step)\n",
        "\n",
        "        return self.output_layer\n",
        "\n",
        "    def backpropagation(self, expected: np.ndarray) -> None:\n",
        "        for step_number in reversed(range(len(expected) - 1)):\n",
        "            delta_output = self.output_layer.backward(\n",
        "                expected[step_number],\n",
        "                self.hidden_layer.get_hidden_state(step_number),\n",
        "                step_number,\n",
        "            )\n",
        "            delta_weighted_sum = self.hidden_layer.backward(\n",
        "                step_number, delta_output\n",
        "            )\n",
        "            self.input_layer.backward(step_number, delta_weighted_sum)\n",
        "\n",
        "\n",
        "        self.output_layer.update_parameters(self.learning_rate)\n",
        "        self.hidden_layer.update_parameters(self.learning_rate)\n",
        "        self.input_layer.update_parameters(self.learning_rate)\n",
        "\n",
        "    def loss(self, y_hat: List[np.ndarray], y: List[np.ndarray]) -> float:\n",
        "        return -np.mean([np.sum(y[i] * np.log(y_hat[i])) for i in range(len(y))])\n",
        "\n",
        "    def _find_end_of_seq(self, expected: np.ndarray) -> int:\n",
        "        for idx, vector in enumerate(expected):\n",
        "            if np.all(vector == 0):\n",
        "                return idx\n",
        "        return len(expected)\n",
        "\n",
        "    def _reset_states(self):\n",
        "      self.output_layer.__reset_predictions__()\n",
        "      self.hidden_layer.__reset_states__()\n",
        "\n",
        "    def train(self, inputs: np.ndarray, expected: np.ndarray, epochs: int) -> None:\n",
        "        for epoch in range(epochs):\n",
        "            loss_list = []\n",
        "            for idx, input in enumerate(tqdm(inputs)):\n",
        "                end_idx = self._find_end_of_seq(expected[idx])\n",
        "                input = input[:end_idx, :]\n",
        "\n",
        "                y_hats = self.feed_forward(input)\n",
        "                self.backpropagation(expected[idx][:end_idx])\n",
        "\n",
        "                round_loss = self.loss(y_hats.predictions[:end_idx,:,0], expected[idx][:end_idx])\n",
        "\n",
        "                loss_list.append(round_loss)\n",
        "\n",
        "                self._reset_states()\n",
        "\n",
        "                if idx % 100 == 99:\n",
        "                  print(f\"Average Training Loss of Last 100 samples: {np.mean(np.array(loss_list[-100:]))}\")\n",
        "\n",
        "            print(\n",
        "                f\"Epoch Loss: {np.mean(np.array(loss_list))}\"\n",
        "            )\n"
      ],
      "metadata": {
        "id": "vjwSh87BelhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Running the RNN\n",
        "\n",
        "Now we put it all together by initializing and training the RNN model. **Do not edit these hyperparameters**. Please do not change the hyperparameters. If done correctly, your loss should be below 5.0 after the second epoch."
      ],
      "metadata": {
        "id": "AA5e87hCDDRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = RNN(vocab_size=vocab_size, hidden_size=32, max_num_time_steps=maxlen - 1, learning_rate=1e-3)\n",
        "rnn.train(x_train_embeddings, y_train_outputs, epochs=2)"
      ],
      "metadata": {
        "id": "-tp8Grw7erxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2 (30 points in total)\n",
        "- In this question, you will count the number of parameters in GPT-2 and trace the source code of different LLM implementations to better understand the Transformer architecture."
      ],
      "metadata": {
        "id": "iYm16QvVQY21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.1 Understand Transformer with parameter counting exercise (19 points in total)\n",
        "\n",
        "### Letâ€™s count the number of parameters in GPT-2-XL.\n",
        "- You are only allowed to use the variable names defined below to answer the questions, i.e., `n_layers * n_heads` is allowed, while `4 * d_ffn` is not allowed.\n",
        "- For simplicity, please **ignore the bias terms** in all questions."
      ],
      "metadata": {
        "id": "BbTqEUJf_M5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers = 48  # the number of transformer layers (aka. transformer blocks)\n",
        "n_heads = 25   # the number of attention heads in each layer\n",
        "d_model = 1600 # the model dimension\n",
        "d_ffn = 6400   # the FFN (aka. MLP) dimension\n",
        "d_heads = 64   # the attn head dimension\n",
        "n_vocab = 50257 # vocabulary size\n",
        "n_ctx = 1024    # the maximum sequence length the model can process"
      ],
      "metadata": {
        "id": "n9AVWuSx_C70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2.1.1 The input embeddings consists of token embeddings (2 points) and position embeddings (2 points). Count the number of parameters in the two embeddings."
      ],
      "metadata": {
        "id": "Yx_V_P93_Y5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings =\n",
        "print(token_embeddings)"
      ],
      "metadata": {
        "id": "DQeVyMUt_V3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position_embeddings =  # Hint: n_ctx\n",
        "print(position_embeddings)"
      ],
      "metadata": {
        "id": "TneTC1eP_jRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2.1.2 Multi-Headed Attention consists of W_Q (1.5 point), W_K (1.5 point), W_V (1.5 points), and W_O (3.5 points),\n",
        "- MultiHead(Q, K, V) = Concat(head_1, ..., head_n) W_O\n",
        "- see more details in https://arxiv.org/pdf/1706.03762\n",
        "#### Count the number of parameters in them.\n",
        "- Here we * n_layers to calculate the total number of parameters across layers."
      ],
      "metadata": {
        "id": "E_yaWxaE_fWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_q = n_layers *\n",
        "print(attn_q)"
      ],
      "metadata": {
        "id": "IO_JpGDB_ebx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_k = n_layers *\n",
        "print(attn_k)"
      ],
      "metadata": {
        "id": "VcHxS114__nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_v = n_layers *\n",
        "print(attn_v)"
      ],
      "metadata": {
        "id": "sYXz2_jBADJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_o = n_layers *\n",
        "print(attn_o)"
      ],
      "metadata": {
        "id": "y45J1CNnAIJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2.1.3 The feed-forward network (FFN) in each transformer block consists of two layers, ffn1 (1.5 point) and ffn2 (1.5 point). Count the number of parameters in them, respectively.\n",
        "- You need to * n_layers to calculate the total number of parameters across layers."
      ],
      "metadata": {
        "id": "aUT0PRLgA2qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffn1 =\n",
        "print(ffn1)"
      ],
      "metadata": {
        "id": "ez8NPTMcAzps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn2 =\n",
        "print(ffn2)"
      ],
      "metadata": {
        "id": "e3ISg8S_AwDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2.1.4 Count the number of parameters in the output embeddings (2 points)"
      ],
      "metadata": {
        "id": "3PTf2vMhBNqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_embeddings =\n",
        "print(output_embeddings)"
      ],
      "metadata": {
        "id": "auX4xsiwBW-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2.1.5 Print out the total number of parameters (1 point).\n",
        "- We do not double-count output_embeddings because GPT-2 shares the weights of token_embeddings and output_embeddings.\n",
        "- For simplicity, we ignore the bias terms and layernorms."
      ],
      "metadata": {
        "id": "HIUo-IIbCTZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_total = token_embeddings + position_embeddings + attn_q + attn_k + attn_v + attn_o + ffn1 + ffn2\n",
        "print(f'{n_total/10**9:.3f}B')"
      ],
      "metadata": {
        "id": "31Tn7OxmCZwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2.1.6 The majority of parameters are in the FFN layers! Print the percentage (1 point)."
      ],
      "metadata": {
        "id": "tmDac9HhBWDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{(ffn1+ffn2)/n_total:.1%}')"
      ],
      "metadata": {
        "id": "Z7xNJNoLCajv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.2 Understand different implementations of large language models (11 points in total)\n",
        "\n",
        "### Different LLMs use slightly different architectures. Please read the snippets of three types of implentations below and answer the questions.\n",
        "- The code is modified from Hugging Face's implementations. To answer the questions, you do not need to understand the classes and variables that are not defined in the snippets.\n",
        "- `d_model`: the model dimension\n",
        "- `d_ffn`: the FFN (aka. MLP) dimension"
      ],
      "metadata": {
        "id": "g_cCCCpdDB99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "class GPT2MLP(nn.Module):\n",
        "    def __init__(self, config, d_model, d_ffn):\n",
        "        super().__init__()\n",
        "        self.fn1 = nn.Linear(d_model, d_ffn)\n",
        "        self.fn2 = nn.Linear(d_ffn, d_model)\n",
        "        self.act = ACT2FN[config.activation_function]\n",
        "        self.dropout = nn.Dropout(config.resid_pdrop)\n",
        "\n",
        "    def forward(self, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatTensor:\n",
        "        hidden_states = self.fn1(hidden_states)\n",
        "        hidden_states = self.act(hidden_states)\n",
        "        hidden_states = self.fn2(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class GPT2Block(nn.Module):\n",
        "    def __init__(self, config, d_model, d_ffn, layer_idx=None):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(d_model, eps=config.layer_norm_epsilon)\n",
        "        self.attn = GPT2_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n",
        "        self.ln_2 = nn.LayerNorm(d_model, eps=config.layer_norm_epsilon)\n",
        "        self.mlp = GPT2MLP(config, d_model, d_ffn)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    ):\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_1(hidden_states)\n",
        "        attn_output = self.attn(\n",
        "            hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "        # residual connection\n",
        "        hidden_states = attn_output + residual\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_2(hidden_states)\n",
        "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
        "        # residual connection\n",
        "        hidden_states = residual + feed_forward_hidden_states\n",
        "\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "RO82AWr9DRqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f118b4ec-2367-44f2-dd8b-a1524d7240ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "skipping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "class GPTJBlock(nn.Module):\n",
        "    def __init__(self, config, d_model, d_ffn, layer_idx=None):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(d_model, eps=config.layer_norm_epsilon)\n",
        "        self.attn = GPTJ_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n",
        "        self.mlp = GPTJMLP(config, d_model, d_ffn)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: Optional[torch.FloatTensor],\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "    ):\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.ln_1(hidden_states)\n",
        "        attn_output = self.attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
        "        # residual connection\n",
        "        hidden_states = attn_output + feed_forward_hidden_states + residual\n",
        "\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "uzQFIbWYDT0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%script echo skipping\n",
        "class LlamaMLP(nn.Module):\n",
        "    def __init__(self, config, d_model, d_ffn):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(d_model, d_ffn, bias=config.mlp_bias)\n",
        "        self.up_proj = nn.Linear(d_model, d_ffn, bias=config.mlp_bias)\n",
        "        self.down_proj = nn.Linear(d_ffn, d_model, bias=config.mlp_bias)\n",
        "        self.act_fn = ACT2FN[config.hidden_act]\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [BS, d_model]\n",
        "        hidden_states = self.act_fn(self.gate_proj(x)) * self.up_proj(x)\n",
        "        output = self.down_proj(hidden_states)\n",
        "        return output"
      ],
      "metadata": {
        "id": "c4lRLiVKDWLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2.2.1 What are the differences between `GPTJBlock()` and `GPT2Block()`? (7 points)\n",
        "- Hint: (1) When do self.attn() and self.mlp() add to the residual stream, respectively? (2) What's the input of self.mlp()?\n",
        "- **Ignore** the differences in bias terms, layernorm, dropout, and activation functions."
      ],
      "metadata": {
        "id": "JzDenRPnDYMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You answer here (double-clik to edit). Please explain things in Enghlish instead of Python."
      ],
      "metadata": {
        "id": "6Pn_Xy3qDhrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q2.2.2 In `LlamaMLP()`, the shape of the input `x` is `[BS, d_model]`. What are the shapes of `self.act_fn(self.gate_proj(x))`, `self.up_proj(x)`, `hidden_states`, and `output`, respectively? Answer with the variable names (4 points).\n",
        "- The type of the activation function (ReLU, SiLU, or Sigmoid) does not change the answer"
      ],
      "metadata": {
        "id": "sSqczliiD-P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Your answer here\n",
        "- (1 point) The shape of the output of `self.act_fn(self.gate_proj(x))`:\n",
        "- (1 point) The shape of the output of `self.up_proj(x)`:\n",
        "- (1 point) The shape of `hidden_states`:\n",
        "- (1 point) The shape of `output`:"
      ],
      "metadata": {
        "id": "ynke0xJJELNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: Finetuning\n",
        "\n",
        "In this question, you will be asked to finetune a pretrained language model DistillBERT on sentiment analysis task using IMDb dataset."
      ],
      "metadata": {
        "id": "ZZFhYECBGFuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "O3W2F3rR8zmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load dataset\n",
        "# we use imdb dataset to finetune the model. We test the model on imdb and sst2.\n",
        "from datasets import load_dataset\n",
        "\n",
        "imdb_dataset = load_dataset('stanfordnlp/imdb')\n"
      ],
      "metadata": {
        "id": "g-ydErHGG-Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preprocess the data (4pts)\n",
        "\n",
        "\n",
        "For imdb, split the train dataset into train set and dev set. The dev set is used within training process to select the best checkpoint. Use train_test_split() from sklearn to split. The ratio of dev set is 0.1. Set the random state as 42."
      ],
      "metadata": {
        "id": "hCdtYhZfIUbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# write your code here, make sure you use the name defined below.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrVgOCBEITjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_x[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0-NstVLwf9H",
        "outputId": "9d3203df-9855-4ea5-b355-47dc0dca1863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Algie, the Miner\" is one bad and unfunny silent comedy. The timing of the slapstick is completely off. This is the kind of humor with certain sequences that make you wonder if they're supposed to be funny or not. However, the actual quality of the film is irrelevant. This is mandatory viewing for film buffs mainly because its one of the earliest examples of gay cinema. The main character of Algie is an effeminate guy, acting much like the stereotypical \"pansy\" common in many early films. The film has the homophobic attitude common of the time. \"Algie, the Miner\" is pretty awful, but fascinating from a historical viewpoint. (3/10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare the data (10pts)\n",
        "\n",
        "\n",
        "We use Dataset class from torch.utils.data to prepare data, and DataLoader class to prepare batches for training.\n",
        "\n",
        "In the SentimentAnalysisDataset class, you need to use DistillBert tokenizer to tokenize the sentence."
      ],
      "metadata": {
        "id": "kw7QOg__Lz7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7F7rgxCPO5JO",
        "outputId": "93a1f2c6-0b43-40b0-8b6d-f703eff62f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "class SentimentAnalysisDataset(Dataset):\n",
        "  #write your code here\n",
        "  def __init__(self,data, tokenizer, max_len = 512):\n",
        "\n",
        "\n",
        "\n",
        "  def prepare(self):\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.len\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "\n",
        "\n",
        "# Example of usage\n",
        "# Usage of GPU: due to limit usage of GPU on Colab, we will not train the whole training set. If you can get access to GPU, we strongly recommend you to run it on GPU and try it on the whole dataset. In this homework, we only run first 20 samples.\n",
        "train = {'input':train_x[:20], 'label':train_y[:20]}\n",
        "train_dataset = SentimentAnalysisDataset(train, tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = 4, shuffle = True)\n"
      ],
      "metadata": {
        "id": "UAWZHrraLzEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717fda76-69d1-4a90-d846-560b28a699d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert sum(train_dataset.attention_masks[0])==149"
      ],
      "metadata": {
        "id": "FeRXj-Hgw-vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Define the model (12pts)\n",
        "\n",
        "We use DistillBERT as base model. We still need a linear layer to mapping the last hidden state to classes dimension.\n",
        "\n",
        "The model should have a base model, a linear layer, a dropout layer (0.5) and softmax function. The forward function go through all the layers one by one and return the softmax result."
      ],
      "metadata": {
        "id": "WRFvGe7p11TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "bertmodel = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "class ClassificationModel(nn.Module):\n",
        "    def __init__(self,base_model,num_classes):\n",
        "        super().__init__()\n",
        "        #write your code here\n",
        "\n",
        "\n",
        "    def forward(self,input_ids, attention_mask):\n",
        "        #write your code here\n",
        "\n",
        "        return predicts"
      ],
      "metadata": {
        "id": "WUl4nHap1fSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ClassificationModel(base_model = bertmodel, num_classes = 2 )"
      ],
      "metadata": {
        "id": "_rrqu5dB2wXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test case\n",
        "input_ids = train_dataset.input_ids[0]\n",
        "attention_mask = train_dataset.attention_masks[0]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  predicts = model(input_ids,attention_mask)\n"
      ],
      "metadata": {
        "id": "vHjc-17g3Dx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert predicts.tolist() == [[0.5158617496490479, 0.48413828015327454]]"
      ],
      "metadata": {
        "id": "1mspVXim3Yk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model finetuning (10pts)\n",
        "\n",
        "In this section, you need to implement train loop and evaluation loop."
      ],
      "metadata": {
        "id": "5_Vbj3xMLJr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Initialize the optimizer. We use AdamW for optimizer and cross entropy loss. (3pts)"
      ],
      "metadata": {
        "id": "P1_w9rx9Ldqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "\n"
      ],
      "metadata": {
        "id": "X7g8NKCILc0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use pytorch to implement. For each epoch, we run one training loop and one evaluation loop. At the end of training, we run the model on test set using the best model saved. For one training step, we run forward pass using pretrained model given input. Then we calculate loss and do backward propagation. See the instructions in the code block.  (7pts)\n",
        "\n",
        "Ideally, you should see a obvious decrease (0.7-1) in train loss, but no decrease in dev loss, since the model is overfitting on a small training set.\n"
      ],
      "metadata": {
        "id": "JRnQzQbhOSs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "epochs = 10 # don't change\n",
        "\n",
        "num_training_steps = epochs * len(train_dataloader)\n",
        "best_model = None\n",
        "with tqdm(total=num_training_steps, desc='Finetuning:') as pbar:\n",
        "  for epoch in range(epochs):\n",
        "    # training loop\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_dataloader:\n",
        "      '''\n",
        "      Tips:\n",
        "      1. Put the input and model on the same device\n",
        "      2. Use the optimizer correctly\n",
        "      3. Update the train loss. The printed train loss should be train_loss/len(train_dataloader)\n",
        "      '''\n",
        "      # write your code here\n",
        "\n",
        "    print(f'Epoch {epoch}: train loss is {train_loss}')\n",
        "    best_loss = 10000\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for batch in dev_dataloader:\n",
        "\n",
        "        dev_loss = 0\n",
        "        '''\n",
        "        Tips:\n",
        "        1. You don't need to use optimizer\n",
        "        2. Update the dev loss. The printed dev loss should be dev_loss/len(dev_dataloader)\n",
        "        3. Save the checkpoint if the dev loss is smaller than best loss and update the best loss to dev loss\n",
        "        '''\n",
        "\n",
        "      print(f'Epoch {epoch}: dev loss is {dev_loss}')\n",
        "      if dev_loss < best_loss:\n",
        "        #save the checkpoint\n",
        "        # write your code here\n",
        "        print(f'The best loss is {dev_loss}. Saving checkpoint!')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eCeuEA4nLawd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TBmLJ0vASj9C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}